2022-11-21 11:41:34,140	INFO trainable.py:160 -- Trainable.setup took 66.163 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2022-11-21 11:41:34,143	WARNING util.py:65 -- Install gputil for GPU system monitoring.
Main training step 0
[36m(RolloutWorker pid=1700)[39m tensor([[0.6878, 0.8390, 0.8390, 0.6878],
[36m(RolloutWorker pid=1700)[39m         [0.8390, 0.0000, 0.9891, 0.8390],
[36m(RolloutWorker pid=1700)[39m         [1.3870, 0.9891, 0.0000, 0.9891],
[36m(RolloutWorker pid=1700)[39m         [1.6350, 1.3870, 0.9891, 0.6878]], grad_fn=<SelectBackward0>)
[36m(RolloutWorker pid=1700)[39m dim4 is not zero
[36m(RolloutWorker pid=1700)[39m tensor([[0.6878, 0.8390, 0.9891, 0.6878],
[36m(RolloutWorker pid=1700)[39m         [0.8390, 0.9891, 0.0000, 0.9891],
[36m(RolloutWorker pid=1700)[39m         [1.3870, 0.0000, 0.9891, 0.8390],
[36m(RolloutWorker pid=1700)[39m         [1.6350, 1.3870, 0.8390, 0.6878]], grad_fn=<SelectBackward0>)
[36m(RolloutWorker pid=1700)[39m dim4 is not zero
tensor([[0.0000, 0.2153, 0.2153, 0.0000],
        [0.2153, 0.0000, 0.0000, 0.2924],
        [0.0000, 0.0000, 0.2924, 0.6535],
        [0.0000, 0.0000, 0.2153, 0.2695]])
dim4 is not zero
[36m(RolloutWorker pid=1700)[39m tensor([[0.6878, 0.8390, 0.9891, 0.6878],
[36m(RolloutWorker pid=1700)[39m         [0.8390, 0.9891, 0.0000, 0.9891],
[36m(RolloutWorker pid=1700)[39m         [1.3870, 0.0000, 0.9891, 0.8390],
[36m(RolloutWorker pid=1700)[39m         [1.6350, 1.3870, 0.8390, 0.6878]], grad_fn=<SelectBackward0>)
[36m(RolloutWorker pid=1700)[39m dim4 is not zero
[36m(RolloutWorker pid=1700)[39m tensor([[0.6878, 0.8390, 0.9891, 0.6878],
[36m(RolloutWorker pid=1700)[39m         [0.8390, 0.9891, 0.0000, 0.9891],
[36m(RolloutWorker pid=1700)[39m         [1.3870, 0.0000, 0.9891, 0.8390],
[36m(RolloutWorker pid=1700)[39m         [1.6350, 1.3870, 0.8390, 0.6878]], grad_fn=<SelectBackward0>)
[36m(RolloutWorker pid=1700)[39m dim4 is not zero
[36m(RolloutWorker pid=1700)[39m tensor([[0.6878, 0.8390, 0.8390, 0.6878],
[36m(RolloutWorker pid=1700)[39m         [0.8390, 0.0000, 0.9891, 0.8390],
[36m(RolloutWorker pid=1700)[39m         [1.3870, 0.9891, 0.0000, 0.9891],
[36m(RolloutWorker pid=1700)[39m         [1.6350, 1.3870, 0.9891, 0.6878]], grad_fn=<SelectBackward0>)
[36m(RolloutWorker pid=1700)[39m dim4 is not zero
[36m(RolloutWorker pid=1700)[39m tensor([[0.6878, 0.8390, 0.8390, 0.6878],
[36m(RolloutWorker pid=1700)[39m         [0.8390, 0.0000, 0.9891, 0.8390],
[36m(RolloutWorker pid=1700)[39m         [1.3870, 0.9891, 0.0000, 0.9891],
[36m(RolloutWorker pid=1700)[39m         [1.6350, 1.3870, 0.9891, 0.6878]], grad_fn=<SelectBackward0>)
[36m(RolloutWorker pid=1700)[39m dim4 is not zero
[36m(RolloutWorker pid=1700)[39m tensor([[0.6878, 0.8390, 0.9891, 0.6878],
[36m(RolloutWorker pid=1700)[39m         [0.8390, 0.9891, 0.0000, 0.9891],
[36m(RolloutWorker pid=1700)[39m         [1.3870, 0.0000, 0.9891, 0.8390],
[36m(RolloutWorker pid=1700)[39m         [1.6350, 1.3870, 0.8390, 0.6878]], grad_fn=<SelectBackward0>)
[36m(RolloutWorker pid=1700)[39m dim4 is not zero
[36m(RolloutWorker pid=1700)[39m tensor([[0.6878, 0.8390, 0.8390, 0.6878],
[36m(RolloutWorker pid=1700)[39m         [0.8390, 0.0000, 0.9891, 0.8390],
[36m(RolloutWorker pid=1700)[39m         [1.3870, 0.9891, 0.0000, 0.9891],
[36m(RolloutWorker pid=1700)[39m         [1.6350, 1.3870, 0.9891, 0.6878]], grad_fn=<SelectBackward0>)
[36m(RolloutWorker pid=1700)[39m dim4 is not zero
tensor([[0.0000, 0.2153, 0.2153, 0.0000],
        [0.2153, 0.0000, 0.2924, 0.2153],
        [0.0000, 0.2924, 0.4523, 0.2924],
        [0.0000, 0.0000, 0.2924, 0.0000]])
2022-11-21 11:41:41,122	WARNING algorithm.py:2178 -- Worker crashed during training or evaluation! To try to continue without failed worker(s), set `ignore_worker_failures=True`. To try to recover the failed worker(s), set `recreate_failed_workers=True`.
dim4 is not zero
[36m(RolloutWorker pid=1700)[39m tensor([[0.6878, 0.8390, 0.8390, 0.6878],
[36m(RolloutWorker pid=1700)[39m         [0.8390, 0.0000, 0.9891, 0.8390],
[36m(RolloutWorker pid=1700)[39m         [1.3870, 0.9891, 0.0000, 0.9891],
[36m(RolloutWorker pid=1700)[39m         [1.6350, 1.3870, 0.9891, 0.6878]], grad_fn=<SelectBackward0>)
[36m(RolloutWorker pid=1700)[39m dim4 is not zero
[36m(RolloutWorker pid=1700)[39m tensor([[0.6878, 0.8390, 0.8390, 0.6878],
[36m(RolloutWorker pid=1700)[39m         [0.8390, 0.0000, 0.0000, 0.9891],
[36m(RolloutWorker pid=1700)[39m         [1.3870, 0.0000, 0.9891, 0.2155],
[36m(RolloutWorker pid=1700)[39m         [1.6350, 1.3870, 0.8390, 1.6769]], grad_fn=<SelectBackward0>)
[36m(RolloutWorker pid=1700)[39m dim4 is not zero
[36m(RolloutWorker pid=1700)[39m tensor([[0.6878, 0.9891, 0.8390, 0.6878],
[36m(RolloutWorker pid=1700)[39m         [0.9891, 0.0000, 0.9891, 0.8390],
[36m(RolloutWorker pid=1700)[39m         [1.3870, 0.9891, 0.0000, 0.8390],
[36m(RolloutWorker pid=1700)[39m         [1.6350, 1.3870, 0.8390, 0.6878]])
[36m(RolloutWorker pid=1700)[39m dim4 is not zero
[36m(RolloutWorker pid=1700)[39m tensor([[1.6769, 0.2155, 0.9891, 0.6878],
[36m(RolloutWorker pid=1700)[39m         [0.8390, 0.9891, 0.0000, 0.8390],
[36m(RolloutWorker pid=1700)[39m         [1.3870, 0.0000, 0.0000, 0.8390],
[36m(RolloutWorker pid=1700)[39m         [1.6350, 1.3870, 0.8390, 0.6878]])
[36m(RolloutWorker pid=1700)[39m dim4 is not zero
[36m(RolloutWorker pid=1700)[39m tensor([[1.6769, 0.2155, 0.9891, 0.6878],
[36m(RolloutWorker pid=1700)[39m         [0.8390, 0.9891, 0.0000, 0.8390],
[36m(RolloutWorker pid=1700)[39m         [1.3870, 0.0000, 0.0000, 0.8390],
[36m(RolloutWorker pid=1700)[39m         [1.6350, 1.3870, 0.8390, 0.6878]])
[36m(RolloutWorker pid=1700)[39m dim4 is not zero
[36m(RolloutWorker pid=1700)[39m tensor([[0.0000, 0.9891, 0.8390, 0.6878],
[36m(RolloutWorker pid=1700)[39m         [0.9891, 0.0000, 0.0000, 0.8390],
[36m(RolloutWorker pid=1700)[39m         [1.3870, 0.0000, 0.0000, 0.8390],
[36m(RolloutWorker pid=1700)[39m         [1.6350, 1.3870, 0.8390, 0.6878]])
[36m(RolloutWorker pid=1700)[39m dim4 is not zero
[36m(RolloutWorker pid=1700)[39m tensor([[0.0000, 0.9891, 0.8390, 0.6878],
[36m(RolloutWorker pid=1700)[39m         [0.9891, 0.0000, 0.0000, 0.8390],
[36m(RolloutWorker pid=1700)[39m         [1.3870, 0.0000, 0.0000, 0.8390],
[36m(RolloutWorker pid=1700)[39m         [1.6350, 1.3870, 0.8390, 0.6878]])
[36m(RolloutWorker pid=1700)[39m dim4 is not zero
[36m(RolloutWorker pid=1700)[39m tensor([[0.0000, 0.9891, 0.8390, 0.6878],
[36m(RolloutWorker pid=1700)[39m         [0.9891, 0.0000, 0.0000, 0.8390],
[36m(RolloutWorker pid=1700)[39m         [1.3870, 0.0000, 0.0000, 0.8390],
[36m(RolloutWorker pid=1700)[39m         [1.6350, 1.3870, 0.8390, 0.6878]])
[36m(RolloutWorker pid=1700)[39m dim4 is not zero
[36m(RolloutWorker pid=1700)[39m tensor([[0.0000, 0.9891, 0.8390, 0.6878],
[36m(RolloutWorker pid=1700)[39m         [0.9891, 0.0000, 0.0000, 0.8390],
[36m(RolloutWorker pid=1700)[39m         [1.3870, 0.0000, 0.0000, 0.8390],
[36m(RolloutWorker pid=1700)[39m         [1.6350, 1.3870, 0.8390, 0.6878]])
[36m(RolloutWorker pid=1700)[39m dim4 is not zero
[36m(RolloutWorker pid=1700)[39m tensor([[0.0000, 0.9891, 0.8390, 0.6878],
[36m(RolloutWorker pid=1700)[39m         [0.9891, 0.0000, 0.0000, 0.8390],
[36m(RolloutWorker pid=1700)[39m         [1.3870, 0.0000, 0.0000, 0.8390],
[36m(RolloutWorker pid=1700)[39m         [1.6350, 1.3870, 0.8390, 0.6878]])
[36m(RolloutWorker pid=1700)[39m dim4 is not zero
[36m(RolloutWorker pid=1700)[39m tensor([[0.0000, 0.9891, 0.8390, 0.6878],
[36m(RolloutWorker pid=1700)[39m         [0.9891, 0.0000, 0.0000, 0.8390],
[36m(RolloutWorker pid=1700)[39m         [1.3870, 0.0000, 0.0000, 0.8390],
[36m(RolloutWorker pid=1700)[39m         [1.6350, 1.3870, 0.8390, 0.6878]])
[36m(RolloutWorker pid=1700)[39m dim4 is not zero
[36m(RolloutWorker pid=1700)[39m tensor([[1.6769, 0.8390, 0.8390, 0.6878],
[36m(RolloutWorker pid=1700)[39m         [0.2155, 0.9891, 0.0000, 0.8390],
[36m(RolloutWorker pid=1700)[39m         [1.3870, 0.0000, 0.0000, 0.8390],
[36m(RolloutWorker pid=1700)[39m         [1.6350, 1.3870, 0.8390, 0.6878]])
[36m(RolloutWorker pid=1700)[39m dim4 is not zero
[36m(RolloutWorker pid=1700)[39m tensor([[0.0000, 0.9891, 0.8390, 0.6878],
[36m(RolloutWorker pid=1700)[39m         [0.9891, 0.0000, 0.0000, 0.8390],
[36m(RolloutWorker pid=1700)[39m         [1.3870, 0.0000, 0.0000, 0.8390],
[36m(RolloutWorker pid=1700)[39m         [1.6350, 1.3870, 0.8390, 0.6878]], grad_fn=<SelectBackward0>)
[36m(RolloutWorker pid=1700)[39m dim4 is not zero
tensor([[0.0000, 0.2153, 0.2153, 0.0000],
        [0.2153, 0.0000, 0.2924, 0.2153],
        [0.0000, 0.2924, 0.4523, 0.2924],
        [0.0000, 0.0000, 0.2924, 0.0000]])
dim4 is not zero
tensor([[0.0000, 0.2153, 0.2153, 0.0000],
        [0.2153, 0.0000, 0.0000, 0.2924],
        [0.0000, 0.0000, 0.2924, 0.6535],
        [0.0000, 0.0000, 0.2153, 0.2695]])
dim4 is not zero
Traceback (most recent call last):
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None, #here?
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "c:\Users\adams\.vscode\extensions\ms-python.python-2022.18.2\pythonFiles\lib\python\debugpy\__main__.py", line 39, in <module>
    cli.main()
  File "c:\Users\adams\.vscode\extensions\ms-python.python-2022.18.2\pythonFiles\lib\python\debugpy/..\debugpy\server\cli.py", line 430, in main
    run()
  File "c:\Users\adams\.vscode\extensions\ms-python.python-2022.18.2\pythonFiles\lib\python\debugpy/..\debugpy\server\cli.py", line 284, in run_file
    runpy.run_path(target, run_name="__main__")
  File "c:\Users\adams\.vscode\extensions\ms-python.python-2022.18.2\pythonFiles\lib\python\debugpy\_vendored\pydevd\_pydevd_bundle\pydevd_runpy.py", line 321, in run_path
    return _run_module_code(code, init_globals, run_name,
  File "c:\Users\adams\.vscode\extensions\ms-python.python-2022.18.2\pythonFiles\lib\python\debugpy\_vendored\pydevd\_pydevd_bundle\pydevd_runpy.py", line 135, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File "c:\Users\adams\.vscode\extensions\ms-python.python-2022.18.2\pythonFiles\lib\python\debugpy\_vendored\pydevd\_pydevd_bundle\pydevd_runpy.py", line 124, in _run_code
    exec(code, run_globals)
  File "c:\Users\adams\OneDrive\Documents\GitHub\Mavi\src\raya3c\example_vin.py", line 366, in <module>
    my_experiment(1)
  File "c:\Users\adams\OneDrive\Documents\GitHub\Mavi\src\raya3c\example_vin.py", line 342, in my_experiment
    result = trainer.train()
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\ray\tune\trainable\trainable.py", line 347, in train
    result = self.step()
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\ray\rllib\algorithms\algorithm.py", line 661, in step
    results, train_iter_ctx = self._run_one_training_iteration()
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\ray\rllib\algorithms\algorithm.py", line 2378, in _run_one_training_iteration
    num_recreated += self.try_recover_from_step_attempt(
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\ray\rllib\algorithms\algorithm.py", line 2185, in try_recover_from_step_attempt
    raise error
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\ray\rllib\algorithms\algorithm.py", line 2373, in _run_one_training_iteration
    results = self.training_step()
  File "c:\Users\adams\OneDrive\Documents\GitHub\Mavi\src\raya3c\a3c.py", line 219, in training_step
    async_results = self._worker_manager.get_ready()
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\ray\rllib\execution\parallel_requests.py", line 173, in get_ready
    objs = ray.get(ready_requests)
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\ray\_private\client_mode_hook.py", line 105, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\ray\_private\worker.py", line 2275, in get
    raise value.as_instanceof_cause() #runtime error: more than 95% of ram used OR
ray.exceptions.RayTaskError(ValueError): [36mray::RolloutWorker.apply()[39m (pid=1700, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000002B8B04C5160>)
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\autograd\__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor []], which is output 0 of AsStridedBackward0, is at version 1280; expected version 115 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
The above exception was the direct cause of the following exception:
[36mray::RolloutWorker.apply()[39m (pid=1700, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000002B8B04C5160>)
  File "python\ray\_raylet.pyx", line 655, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 696, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 662, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 666, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 613, in ray._raylet.execute_task.function_executor
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\ray\_private\function_manager.py", line 674, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\ray\util\tracing\tracing_helper.py", line 466, in _resume_span
    return method(self, *_args, **_kwargs)
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\ray\rllib\evaluation\rollout_worker.py", line 1664, in apply
    return func(self, *args, **kwargs)
  File "c:\Users\adams\OneDrive\Documents\GitHub\Mavi\src\raya3c\a3c.py", line 200, in sample_and_compute_grads
    grads, infos = worker.compute_gradients(samples)
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\ray\util\tracing\tracing_helper.py", line 466, in _resume_span
    return method(self, *_args, **_kwargs)
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\ray\rllib\evaluation\rollout_worker.py", line 1043, in compute_gradients
    grad_out[pid], info_out[pid] = self.policy_map[pid].compute_gradients(
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\ray\rllib\utils\threading.py", line 24, in wrapper
    return func(self, *a, **k)
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\ray\rllib\policy\torch_policy_v2.py", line 789, in compute_gradients
    tower_outputs = self._multi_gpu_parallel_grad_calc([postprocessed_batch])
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\ray\rllib\policy\torch_policy_v2.py", line 1179, in _multi_gpu_parallel_grad_calc
    raise last_result[0] from last_result[1]
ValueError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor []], which is output 0 of AsStridedBackward0, is at version 1280; expected version 115 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
 tracebackTraceback (most recent call last):
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\ray\rllib\policy\torch_policy_v2.py", line 1117, in _worker
    loss_out[opt_idx].backward(retain_graph=True)
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\autograd\__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor []], which is output 0 of AsStridedBackward0, is at version 1280; expected version 115 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
In tower 0 on device cpu
Traceback (most recent call last):
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None, #here?
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "c:\Users\adams\.vscode\extensions\ms-python.python-2022.18.2\pythonFiles\lib\python\debugpy\__main__.py", line 39, in <module>
    cli.main()
  File "c:\Users\adams\.vscode\extensions\ms-python.python-2022.18.2\pythonFiles\lib\python\debugpy/..\debugpy\server\cli.py", line 430, in main
    run()
  File "c:\Users\adams\.vscode\extensions\ms-python.python-2022.18.2\pythonFiles\lib\python\debugpy/..\debugpy\server\cli.py", line 284, in run_file
    runpy.run_path(target, run_name="__main__")
  File "c:\Users\adams\.vscode\extensions\ms-python.python-2022.18.2\pythonFiles\lib\python\debugpy\_vendored\pydevd\_pydevd_bundle\pydevd_runpy.py", line 321, in run_path
    return _run_module_code(code, init_globals, run_name,
  File "c:\Users\adams\.vscode\extensions\ms-python.python-2022.18.2\pythonFiles\lib\python\debugpy\_vendored\pydevd\_pydevd_bundle\pydevd_runpy.py", line 135, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File "c:\Users\adams\.vscode\extensions\ms-python.python-2022.18.2\pythonFiles\lib\python\debugpy\_vendored\pydevd\_pydevd_bundle\pydevd_runpy.py", line 124, in _run_code
    exec(code, run_globals)
  File "c:\Users\adams\OneDrive\Documents\GitHub\Mavi\src\raya3c\example_vin.py", line 366, in <module>
    my_experiment(1)
  File "c:\Users\adams\OneDrive\Documents\GitHub\Mavi\src\raya3c\example_vin.py", line 342, in my_experiment
    result = trainer.train()
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\ray\tune\trainable\trainable.py", line 347, in train
    result = self.step()
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\ray\rllib\algorithms\algorithm.py", line 661, in step
    results, train_iter_ctx = self._run_one_training_iteration()
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\ray\rllib\algorithms\algorithm.py", line 2378, in _run_one_training_iteration
    num_recreated += self.try_recover_from_step_attempt(
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\ray\rllib\algorithms\algorithm.py", line 2185, in try_recover_from_step_attempt
    raise error
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\ray\rllib\algorithms\algorithm.py", line 2373, in _run_one_training_iteration
    results = self.training_step()
  File "c:\Users\adams\OneDrive\Documents\GitHub\Mavi\src\raya3c\a3c.py", line 219, in training_step
    async_results = self._worker_manager.get_ready()
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\ray\rllib\execution\parallel_requests.py", line 173, in get_ready
    objs = ray.get(ready_requests)
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\ray\_private\client_mode_hook.py", line 105, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\ray\_private\worker.py", line 2275, in get
    raise value.as_instanceof_cause() #runtime error: more than 95% of ram used OR
ray.exceptions.RayTaskError(ValueError): [36mray::RolloutWorker.apply()[39m (pid=1700, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000002B8B04C5160>)
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\autograd\__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor []], which is output 0 of AsStridedBackward0, is at version 1280; expected version 115 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
The above exception was the direct cause of the following exception:
[36mray::RolloutWorker.apply()[39m (pid=1700, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000002B8B04C5160>)
  File "python\ray\_raylet.pyx", line 655, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 696, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 662, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 666, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 613, in ray._raylet.execute_task.function_executor
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\ray\_private\function_manager.py", line 674, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\ray\util\tracing\tracing_helper.py", line 466, in _resume_span
    return method(self, *_args, **_kwargs)
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\ray\rllib\evaluation\rollout_worker.py", line 1664, in apply
    return func(self, *args, **kwargs)
  File "c:\Users\adams\OneDrive\Documents\GitHub\Mavi\src\raya3c\a3c.py", line 200, in sample_and_compute_grads
    grads, infos = worker.compute_gradients(samples)
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\ray\util\tracing\tracing_helper.py", line 466, in _resume_span
    return method(self, *_args, **_kwargs)
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\ray\rllib\evaluation\rollout_worker.py", line 1043, in compute_gradients
    grad_out[pid], info_out[pid] = self.policy_map[pid].compute_gradients(
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\ray\rllib\utils\threading.py", line 24, in wrapper
    return func(self, *a, **k)
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\ray\rllib\policy\torch_policy_v2.py", line 789, in compute_gradients
    tower_outputs = self._multi_gpu_parallel_grad_calc([postprocessed_batch])
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\ray\rllib\policy\torch_policy_v2.py", line 1179, in _multi_gpu_parallel_grad_calc
    raise last_result[0] from last_result[1]
ValueError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor []], which is output 0 of AsStridedBackward0, is at version 1280; expected version 115 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
 tracebackTraceback (most recent call last):
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\ray\rllib\policy\torch_policy_v2.py", line 1117, in _worker
    loss_out[opt_idx].backward(retain_graph=True)
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "C:\Users\adams\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\autograd\__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor []], which is output 0 of AsStridedBackward0, is at version 1280; expected version 115 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
In tower 0 on device cpu