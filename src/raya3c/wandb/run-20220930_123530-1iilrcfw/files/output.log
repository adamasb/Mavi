{'episode_reward': 25.738095238095237, 'episode_lengths': 25.738095238095237, 'training_iteration_time_ms': 1.128, 'grad_wait_time_ms': 0.719, 'apply_grad_time_ms': 1.145, 'apply_grad_throughput': 8729.949, 'synch_weights_time_ms': 1.462, 'num_env_steps_sampled': 1080, 'num_env_steps_trained': 1080, 'num_agent_steps_sampled': 1080, 'num_agent_steps_trained': 1080}
training epoch 0 42 66.0 25.738095238095237
[33m(raylet)[39m [2022-09-30 12:35:33,723 E 9684 9711] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2022-09-30_12-35-11_067290_9592 is over 95% full, available space: 4587614208; capacity: 125844406272. Object creation will fail if spilling is required.
{'episode_reward': 24.213333333333335, 'episode_lengths': 24.213333333333335, 'training_iteration_time_ms': 1.305, 'grad_wait_time_ms': 1.268, 'apply_grad_time_ms': 1.166, 'apply_grad_throughput': 8579.414, 'synch_weights_time_ms': 0.784, 'num_env_steps_sampled': 1810, 'num_env_steps_trained': 1810, 'num_agent_steps_sampled': 1810, 'num_agent_steps_trained': 1810}
training epoch 1 75 66.0 24.213333333333335
{'episode_reward': 21.85, 'episode_lengths': 21.85, 'training_iteration_time_ms': 1.882, 'grad_wait_time_ms': 1.601, 'apply_grad_time_ms': 1.759, 'apply_grad_throughput': 5686.575, 'synch_weights_time_ms': 1.219, 'num_env_steps_sampled': 2690, 'num_env_steps_trained': 2690, 'num_agent_steps_sampled': 2690, 'num_agent_steps_trained': 2690}
training epoch 2 100 44.0 21.85
[33m(raylet)[39m [2022-09-30 12:35:43,744 E 9684 9711] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2022-09-30_12-35-11_067290_9592 is over 95% full, available space: 4590137344; capacity: 125844406272. Object creation will fail if spilling is required.
{'episode_reward': 22.79, 'episode_lengths': 22.79, 'training_iteration_time_ms': 1.967, 'grad_wait_time_ms': 1.345, 'apply_grad_time_ms': 2.52, 'apply_grad_throughput': 3968.534, 'synch_weights_time_ms': 1.208, 'num_env_steps_sampled': 3570, 'num_env_steps_trained': 3570, 'num_agent_steps_sampled': 3570, 'num_agent_steps_trained': 3570}
training epoch 3 100 89.0 22.79
{'episode_reward': 24.73, 'episode_lengths': 24.73, 'training_iteration_time_ms': 1.868, 'grad_wait_time_ms': 1.429, 'apply_grad_time_ms': 2.613, 'apply_grad_throughput': 3826.326, 'synch_weights_time_ms': 2.586, 'num_env_steps_sampled': 4480, 'num_env_steps_trained': 4480, 'num_agent_steps_sampled': 4480, 'num_agent_steps_trained': 4480}
training epoch 4 100 89.0 24.73
[33m(raylet)[39m [2022-09-30 12:35:53,763 E 9684 9711] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2022-09-30_12-35-11_067290_9592 is over 95% full, available space: 4589162496; capacity: 125844406272. Object creation will fail if spilling is required.
{'episode_reward': 27.2, 'episode_lengths': 27.2, 'training_iteration_time_ms': 0.256, 'grad_wait_time_ms': 0.233, 'apply_grad_time_ms': 2.731, 'apply_grad_throughput': 3661.932, 'synch_weights_time_ms': 2.849, 'num_env_steps_sampled': 5190, 'num_env_steps_trained': 5190, 'num_agent_steps_sampled': 5190, 'num_agent_steps_trained': 5190}
training epoch 5 100 119.0 27.2
{'episode_reward': 26.18, 'episode_lengths': 26.18, 'training_iteration_time_ms': 0.228, 'grad_wait_time_ms': 0.207, 'apply_grad_time_ms': 1.989, 'apply_grad_throughput': 5028.659, 'synch_weights_time_ms': 2.74, 'num_env_steps_sampled': 5980, 'num_env_steps_trained': 5980, 'num_agent_steps_sampled': 5980, 'num_agent_steps_trained': 5980}
training epoch 6 100 119.0 26.18
[33m(raylet)[39m [2022-09-30 12:36:03,776 E 9684 9711] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2022-09-30_12-35-11_067290_9592 is over 95% full, available space: 4594196480; capacity: 125844406272. Object creation will fail if spilling is required.
{'episode_reward': 25.39, 'episode_lengths': 25.39, 'training_iteration_time_ms': 0.164, 'grad_wait_time_ms': 0.149, 'apply_grad_time_ms': 1.101, 'apply_grad_throughput': 9084.282, 'synch_weights_time_ms': 0.814, 'num_env_steps_sampled': 6660, 'num_env_steps_trained': 6660, 'num_agent_steps_sampled': 6660, 'num_agent_steps_trained': 6660}
training epoch 7 100 119.0 25.39
{'episode_reward': 23.41, 'episode_lengths': 23.41, 'training_iteration_time_ms': 0.693, 'grad_wait_time_ms': 0.366, 'apply_grad_time_ms': 1.099, 'apply_grad_throughput': 9099.458, 'synch_weights_time_ms': 0.837, 'num_env_steps_sampled': 8200, 'num_env_steps_trained': 8200, 'num_agent_steps_sampled': 8200, 'num_agent_steps_trained': 8200}
training epoch 8 100 73.0 23.41
[33m(raylet)[39m [2022-09-30 12:36:13,784 E 9684 9711] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2022-09-30_12-35-11_067290_9592 is over 95% full, available space: 4601974784; capacity: 125844406272. Object creation will fail if spilling is required.
{'episode_reward': 22.9, 'episode_lengths': 22.9, 'training_iteration_time_ms': 0.165, 'grad_wait_time_ms': 0.152, 'apply_grad_time_ms': 1.11, 'apply_grad_throughput': 9011.482, 'synch_weights_time_ms': 0.783, 'num_env_steps_sampled': 9750, 'num_env_steps_trained': 9750, 'num_agent_steps_sampled': 9750, 'num_agent_steps_trained': 9750}
training epoch 9 100 61.0 22.9
[15.0, 23.0, 33.0, 22.0, 16.0, 15.0, 17.0, 19.0, 23.0, 31.0, 18.0, 18.0, 12.0, 43.0, 17.0, 12.0, 14.0, 17.0, 41.0, 13.0, 46.0, 15.0, 14.0, 26.0, 12.0, 16.0, 29.0, 19.0, 48.0, 36.0, 29.0, 27.0, 28.0, 27.0, 21.0, 19.0, 30.0, 25.0, 10.0, 13.0, 42.0, 13.0, 61.0, 25.0, 16.0, 26.0, 17.0, 14.0, 10.0, 17.0, 12.0, 12.0, 9.0, 25.0, 14.0, 28.0, 16.0, 21.0, 22.0, 9.0, 44.0, 34.0, 13.0, 30.0, 10.0, 18.0, 13.0, 26.0, 16.0, 22.0, 12.0, 48.0, 14.0, 36.0, 19.0, 12.0, 34.0, 28.0, 18.0, 37.0, 32.0, 33.0, 14.0, 16.0, 21.0, 47.0, 15.0, 15.0, 12.0, 26.0, 45.0, 13.0, 19.0, 31.0, 28.0, 14.0, 39.0, 25.0, 12.0, 31.0]
2022-09-30 12:36:21,501	WARNING deprecation.py:47 -- DeprecationWarning: `compute_action` has been deprecated. Use `Trainer.compute_single_action()` instead. This will raise an error in the future!
['self', 'mode']
 10%|â–ˆ         | 1/10 [00:01<00:14,  1.58s/it, Episode=0, Accumulated Reward=16, Average Reward=1, Length=16, Steps=16]



100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:08<00:00,  1.15it/s, Episode=9, Accumulated Reward=13, Average Reward=1, Length=13, Steps=183]
[None]
Job done