2022-09-30 17:02:45,729	INFO trainable.py:160 -- Trainable.setup took 24.909 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2022-09-30 17:02:45,730	WARNING util.py:65 -- Install gputil for GPU system monitoring.
Main trainign step 0
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.931471824645996, 'policy_loss': 36.99603271484375, 'vf_loss': 179.7366180419922, 'total_loss': 126.79502868652344}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 21.66181182861328, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.931117057800293, 'policy_loss': 21.12173843383789, 'vf_loss': 58.437416076660156, 'total_loss': 50.27113342285156}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.93059778213501, 'policy_loss': 36.97237777709961, 'vf_loss': 179.76165771484375, 'total_loss': 126.78390502929688}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.928081035614014, 'policy_loss': 31.66780662536621, 'vf_loss': 133.6791534423828, 'total_loss': 98.43810272216797}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930139541625977, 'policy_loss': 37.02149200439453, 'vf_loss': 180.3036651611328, 'total_loss': 127.1040267944336}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 23.35938262939453, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930549621582031, 'policy_loss': 23.32333755493164, 'vf_loss': 73.82572174072266, 'total_loss': 60.166893005371094}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.929969310760498, 'policy_loss': 36.74634552001953, 'vf_loss': 180.32203674316406, 'total_loss': 126.83806610107422}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 26.100034713745117, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930850028991699, 'policy_loss': 31.24024200439453, 'vf_loss': 134.1719970703125, 'total_loss': 98.2569351196289}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9300079345703125, 'policy_loss': 36.924808502197266, 'vf_loss': 179.50732421875, 'total_loss': 126.60916900634766}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.93046760559082, 'policy_loss': 31.043275833129883, 'vf_loss': 134.1910858154297, 'total_loss': 98.06951141357422}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 15.9969482421875, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9307451248168945, 'policy_loss': 20.51699447631836, 'vf_loss': 53.23020553588867, 'total_loss': 47.06278610229492}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930417060852051, 'policy_loss': 37.117950439453125, 'vf_loss': 179.8720245361328, 'total_loss': 126.98465728759766}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 20.0367431640625, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930966377258301, 'policy_loss': 23.249134063720703, 'vf_loss': 73.72838592529297, 'total_loss': 60.04401779174805}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930231094360352, 'policy_loss': 22.90791130065918, 'vf_loss': 73.2442626953125, 'total_loss': 59.46074295043945}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.929908275604248, 'policy_loss': 36.392276763916016, 'vf_loss': 178.59527587890625, 'total_loss': 125.62061309814453}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9310221672058105, 'policy_loss': 37.05438995361328, 'vf_loss': 179.11590576171875, 'total_loss': 126.54302978515625}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930502891540527, 'policy_loss': 37.0399055480957, 'vf_loss': 179.74636840820312, 'total_loss': 126.84378051757812}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930876731872559, 'policy_loss': 37.11878204345703, 'vf_loss': 179.2740936279297, 'total_loss': 126.6865234375}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 29.21961212158203, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.93068790435791, 'policy_loss': 26.54076385498047, 'vf_loss': 99.9660873413086, 'total_loss': 76.45449829101562}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.929012775421143, 'policy_loss': 36.19655227661133, 'vf_loss': 177.7526092529297, 'total_loss': 125.00357055664062}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.93046236038208, 'policy_loss': 36.765960693359375, 'vf_loss': 179.0769500732422, 'total_loss': 126.2351303100586}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 26.435041427612305, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930553436279297, 'policy_loss': 23.18545913696289, 'vf_loss': 73.70637512207031, 'total_loss': 59.96934127807617}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.931253433227539, 'policy_loss': 37.23525619506836, 'vf_loss': 179.44200134277344, 'total_loss': 126.8869400024414}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.929967403411865, 'policy_loss': 22.909273147583008, 'vf_loss': 73.49966430664062, 'total_loss': 59.589805603027344}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.93021297454834, 'policy_loss': 37.40442657470703, 'vf_loss': 179.75672912597656, 'total_loss': 127.21348571777344}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 20.793128967285156, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930209159851074, 'policy_loss': 26.453638076782227, 'vf_loss': 99.42546081542969, 'total_loss': 76.09706115722656}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.929633140563965, 'policy_loss': 37.87084197998047, 'vf_loss': 179.805908203125, 'total_loss': 127.70449829101562}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9293317794799805, 'policy_loss': 22.76321792602539, 'vf_loss': 73.3656005859375, 'total_loss': 59.37672424316406}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 18.003759384155273, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930419921875, 'policy_loss': 23.164531707763672, 'vf_loss': 73.45967864990234, 'total_loss': 59.82506561279297}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930355548858643, 'policy_loss': 36.945560455322266, 'vf_loss': 179.60491943359375, 'total_loss': 126.6787109375}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 29.283063888549805, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9301276206970215, 'policy_loss': 20.22005844116211, 'vf_loss': 52.863380432128906, 'total_loss': 46.58244705200195}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930959701538086, 'policy_loss': 37.27241134643555, 'vf_loss': 179.6705322265625, 'total_loss': 127.03836822509766}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930642127990723, 'policy_loss': 37.084529876708984, 'vf_loss': 179.39828491210938, 'total_loss': 126.71437072753906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 22.953083038330078, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.929745674133301, 'policy_loss': 20.520450592041016, 'vf_loss': 52.75043487548828, 'total_loss': 46.82637023925781}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.929476261138916, 'policy_loss': 20.86411476135254, 'vf_loss': 57.776939392089844, 'total_loss': 49.683292388916016}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9293975830078125, 'policy_loss': 26.08611297607422, 'vf_loss': 97.97381591796875, 'total_loss': 75.00372314453125}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930649280548096, 'policy_loss': 37.0797004699707, 'vf_loss': 179.8804931640625, 'total_loss': 126.95063781738281}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.931135654449463, 'policy_loss': 37.154197692871094, 'vf_loss': 179.52706909179688, 'total_loss': 126.84841918945312}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.929925441741943, 'policy_loss': 37.042625427246094, 'vf_loss': 179.4495086669922, 'total_loss': 126.69808197021484}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 27.08245086669922, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930697917938232, 'policy_loss': 31.114643096923828, 'vf_loss': 134.49176025390625, 'total_loss': 98.29121398925781}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930190563201904, 'policy_loss': 20.653587341308594, 'vf_loss': 57.210323333740234, 'total_loss': 49.189449310302734}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.931254863739014, 'policy_loss': 37.19389724731445, 'vf_loss': 180.09791564941406, 'total_loss': 127.17354583740234}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930932998657227, 'policy_loss': 37.241249084472656, 'vf_loss': 179.35997009277344, 'total_loss': 126.85192108154297}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930593490600586, 'policy_loss': 36.98575210571289, 'vf_loss': 179.44834899902344, 'total_loss': 126.640625}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930431842803955, 'policy_loss': 30.603126525878906, 'vf_loss': 131.203125, 'total_loss': 96.13538360595703}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930992603302002, 'policy_loss': 30.777788162231445, 'vf_loss': 132.99314880371094, 'total_loss': 97.20504760742188}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.931291103363037, 'policy_loss': 36.84437561035156, 'vf_loss': 177.8369598388672, 'total_loss': 125.69354248046875}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9312286376953125, 'policy_loss': 21.06711196899414, 'vf_loss': 57.85980224609375, 'total_loss': 49.92770004272461}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.931334018707275, 'policy_loss': 37.306697845458984, 'vf_loss': 181.04017639160156, 'total_loss': 127.7574691772461}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 22.288448333740234, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.931109428405762, 'policy_loss': 20.946636199951172, 'vf_loss': 58.01889419555664, 'total_loss': 49.886775970458984}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 19.854494094848633, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930970668792725, 'policy_loss': 20.868017196655273, 'vf_loss': 57.275306701660156, 'total_loss': 49.43635940551758}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930718421936035, 'policy_loss': 35.836448669433594, 'vf_loss': 174.12185668945312, 'total_loss': 122.82807159423828}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930934906005859, 'policy_loss': 36.85630416870117, 'vf_loss': 180.43556213378906, 'total_loss': 127.00476837158203}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 19.8785400390625, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930972099304199, 'policy_loss': 22.778886795043945, 'vf_loss': 72.35350036621094, 'total_loss': 58.88632583618164}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.931347846984863, 'policy_loss': 36.709068298339844, 'vf_loss': 177.01710510253906, 'total_loss': 125.14830780029297}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930764198303223, 'policy_loss': 37.050682067871094, 'vf_loss': 181.42498779296875, 'total_loss': 127.6938705444336}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.929018020629883, 'policy_loss': 37.328094482421875, 'vf_loss': 179.91180419921875, 'total_loss': 127.21470642089844}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9309163093566895, 'policy_loss': 31.008975982666016, 'vf_loss': 132.81271362304688, 'total_loss': 97.34602355957031}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 28.447776794433594, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.931022644042969, 'policy_loss': 23.002338409423828, 'vf_loss': 72.61421966552734, 'total_loss': 59.24013900756836}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930511474609375, 'policy_loss': 37.30093765258789, 'vf_loss': 179.76226806640625, 'total_loss': 127.11276245117188}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.931143760681152, 'policy_loss': 37.06645965576172, 'vf_loss': 177.10426330566406, 'total_loss': 125.54927825927734}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.93113899230957, 'policy_loss': 23.008628845214844, 'vf_loss': 72.44538116455078, 'total_loss': 59.16200637817383}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930480003356934, 'policy_loss': 37.31147766113281, 'vf_loss': 180.54266357421875, 'total_loss': 127.51350402832031}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9300079345703125, 'policy_loss': 37.20867919921875, 'vf_loss': 180.1420440673828, 'total_loss': 127.21040344238281}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 22.187593460083008, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930519104003906, 'policy_loss': 20.74982261657715, 'vf_loss': 56.204734802246094, 'total_loss': 48.78288269042969}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 28.043445587158203, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.931297779083252, 'policy_loss': 22.691186904907227, 'vf_loss': 71.0542984008789, 'total_loss': 58.149024963378906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9313859939575195, 'policy_loss': 36.8376579284668, 'vf_loss': 176.57119750976562, 'total_loss': 125.05394744873047}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 25.907459259033203, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.931352615356445, 'policy_loss': 22.96019172668457, 'vf_loss': 72.64562225341797, 'total_loss': 59.21369171142578}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.931443214416504, 'policy_loss': 36.92253112792969, 'vf_loss': 177.8236083984375, 'total_loss': 125.76502227783203}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 21.74197769165039, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.931450843811035, 'policy_loss': 20.715408325195312, 'vf_loss': 56.00800323486328, 'total_loss': 48.65009689331055}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.931400299072266, 'policy_loss': 36.721954345703125, 'vf_loss': 177.6291046142578, 'total_loss': 125.46719360351562}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 29.164079666137695, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9313201904296875, 'policy_loss': 20.55614471435547, 'vf_loss': 55.120689392089844, 'total_loss': 48.047176361083984}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.931044578552246, 'policy_loss': 36.960453033447266, 'vf_loss': 178.4710235595703, 'total_loss': 126.12665557861328}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 23.581159591674805, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930882930755615, 'policy_loss': 22.90313148498535, 'vf_loss': 72.92098999023438, 'total_loss': 59.294315338134766}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930929183959961, 'policy_loss': 36.81363296508789, 'vf_loss': 178.67906188964844, 'total_loss': 126.08385467529297}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930929183959961, 'policy_loss': 36.905418395996094, 'vf_loss': 177.69970703125, 'total_loss': 125.68595886230469}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930153846740723, 'policy_loss': 37.480751037597656, 'vf_loss': 181.75230407714844, 'total_loss': 128.28759765625}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9304962158203125, 'policy_loss': 24.98248863220215, 'vf_loss': 90.45045471191406, 'total_loss': 70.13841247558594}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930878639221191, 'policy_loss': 36.47072982788086, 'vf_loss': 179.073486328125, 'total_loss': 125.93817138671875}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.929570198059082, 'policy_loss': 30.642717361450195, 'vf_loss': 127.22602844238281, 'total_loss': 94.18643188476562}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.929281234741211, 'policy_loss': 31.24395179748535, 'vf_loss': 137.23776245117188, 'total_loss': 99.79354095458984}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.929545879364014, 'policy_loss': 37.16738510131836, 'vf_loss': 177.69085693359375, 'total_loss': 125.94351959228516}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930168151855469, 'policy_loss': 30.377086639404297, 'vf_loss': 134.78761291503906, 'total_loss': 97.70158386230469}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 25.475143432617188, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.929654121398926, 'policy_loss': 19.175342559814453, 'vf_loss': 47.31645202636719, 'total_loss': 42.7642707824707}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.927896499633789, 'policy_loss': 37.53013610839844, 'vf_loss': 181.03521728515625, 'total_loss': 127.97846221923828}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.927743911743164, 'policy_loss': 37.522308349609375, 'vf_loss': 183.97067260742188, 'total_loss': 129.43836975097656}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9282965660095215, 'policy_loss': 21.95293426513672, 'vf_loss': 64.6627197265625, 'total_loss': 54.21501159667969}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.927666187286377, 'policy_loss': 29.9354190826416, 'vf_loss': 128.96640014648438, 'total_loss': 94.3493423461914}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9310150146484375, 'policy_loss': 33.30561065673828, 'vf_loss': 155.6404266357422, 'total_loss': 111.05651092529297}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.929589748382568, 'policy_loss': 36.85313415527344, 'vf_loss': 181.24078369140625, 'total_loss': 127.40422821044922}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9298996925354, 'policy_loss': 37.168418884277344, 'vf_loss': 180.28024291992188, 'total_loss': 127.23924255371094}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 25.88892364501953, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.929178237915039, 'policy_loss': 19.15256690979004, 'vf_loss': 48.049400329589844, 'total_loss': 43.10797882080078}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930768013000488, 'policy_loss': 36.58689498901367, 'vf_loss': 178.16192626953125, 'total_loss': 125.59855651855469}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.929989337921143, 'policy_loss': 30.4134578704834, 'vf_loss': 135.7257843017578, 'total_loss': 98.2070541381836}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9284772872924805, 'policy_loss': 21.47373390197754, 'vf_loss': 67.4429702758789, 'total_loss': 55.12593460083008}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.926629066467285, 'policy_loss': 37.591941833496094, 'vf_loss': 189.6510772705078, 'total_loss': 132.3482208251953}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930706977844238, 'policy_loss': 36.75080871582031, 'vf_loss': 178.91848754882812, 'total_loss': 126.1407470703125}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.92996072769165, 'policy_loss': 21.380468368530273, 'vf_loss': 69.73382568359375, 'total_loss': 56.17808532714844}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.923334121704102, 'policy_loss': 36.671634674072266, 'vf_loss': 170.3274688720703, 'total_loss': 121.7661361694336}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9181671142578125, 'policy_loss': 38.09095001220703, 'vf_loss': 186.81741333007812, 'total_loss': 131.43048095703125}
No gradient information.s
training epoch 0 52 45.0 19.346153846153847
Main trainign step 1
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.918922424316406, 'policy_loss': 20.714054107666016, 'vf_loss': 51.82306671142578, 'total_loss': 46.556396484375}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9213337898254395, 'policy_loss': 37.170440673828125, 'vf_loss': 182.78916931152344, 'total_loss': 128.49581909179688}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 29.144187927246094, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.923682689666748, 'policy_loss': 19.332067489624023, 'vf_loss': 47.63151550292969, 'total_loss': 43.078590393066406}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.918152332305908, 'policy_loss': 38.029354095458984, 'vf_loss': 182.428955078125, 'total_loss': 129.17465209960938}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.924291610717773, 'policy_loss': 22.98268699645996, 'vf_loss': 73.64852905273438, 'total_loss': 59.737709045410156}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930438995361328, 'policy_loss': 36.6206169128418, 'vf_loss': 178.69747924804688, 'total_loss': 125.90005493164062}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9311203956604, 'policy_loss': 36.56050109863281, 'vf_loss': 177.40052795410156, 'total_loss': 125.19145202636719}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.92838191986084, 'policy_loss': 31.327091217041016, 'vf_loss': 140.5987548828125, 'total_loss': 101.55718231201172}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.926078796386719, 'policy_loss': 18.717275619506836, 'vf_loss': 44.86259841918945, 'total_loss': 41.079315185546875}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.929523468017578, 'policy_loss': 37.04560089111328, 'vf_loss': 182.86383056640625, 'total_loss': 128.40821838378906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930299282073975, 'policy_loss': 26.026016235351562, 'vf_loss': 102.11404418945312, 'total_loss': 77.01373291015625}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.918001651763916, 'policy_loss': 36.94057083129883, 'vf_loss': 169.7675018310547, 'total_loss': 121.75514221191406}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9248881340026855, 'policy_loss': 22.856245040893555, 'vf_loss': 73.11512756347656, 'total_loss': 59.34456253051758}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930004596710205, 'policy_loss': 36.67658615112305, 'vf_loss': 177.66763305664062, 'total_loss': 125.44110870361328}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930923938751221, 'policy_loss': 21.735187530517578, 'vf_loss': 73.2357177734375, 'total_loss': 58.28373718261719}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930077075958252, 'policy_loss': 25.36124038696289, 'vf_loss': 99.4981689453125, 'total_loss': 75.04102325439453}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.928652763366699, 'policy_loss': 34.58824157714844, 'vf_loss': 159.2311553955078, 'total_loss': 114.13452911376953}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.913765907287598, 'policy_loss': 37.72355651855469, 'vf_loss': 170.0245819091797, 'total_loss': 122.66670989990234}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.924511432647705, 'policy_loss': 30.823225021362305, 'vf_loss': 132.02928161621094, 'total_loss': 96.76862335205078}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.919851779937744, 'policy_loss': 34.28963088989258, 'vf_loss': 151.08631896972656, 'total_loss': 109.76359558105469}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.920430660247803, 'policy_loss': 38.031185150146484, 'vf_loss': 172.48428344726562, 'total_loss': 124.2041244506836}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.924759864807129, 'policy_loss': 18.325071334838867, 'vf_loss': 46.29821014404297, 'total_loss': 41.40492630004883}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.928500175476074, 'policy_loss': 36.0703125, 'vf_loss': 184.564697265625, 'total_loss': 128.2833709716797}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930038928985596, 'policy_loss': 34.86893844604492, 'vf_loss': 163.95985412597656, 'total_loss': 116.7795639038086}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930131912231445, 'policy_loss': 21.871274948120117, 'vf_loss': 71.57054138183594, 'total_loss': 57.58724594116211}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9305195808410645, 'policy_loss': 35.118194580078125, 'vf_loss': 166.34390258789062, 'total_loss': 118.22084045410156}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.929438591003418, 'policy_loss': 37.80562210083008, 'vf_loss': 188.4712371826172, 'total_loss': 131.97195434570312}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.928016662597656, 'policy_loss': 18.058921813964844, 'vf_loss': 48.72918701171875, 'total_loss': 42.3542366027832}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.926416873931885, 'policy_loss': 32.39114761352539, 'vf_loss': 137.0682830810547, 'total_loss': 100.85602569580078}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.929620265960693, 'policy_loss': 36.608253479003906, 'vf_loss': 176.25820922851562, 'total_loss': 124.66806030273438}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.926375865936279, 'policy_loss': 18.141098022460938, 'vf_loss': 48.00309371948242, 'total_loss': 42.07338333129883}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.92935848236084, 'policy_loss': 35.984214782714844, 'vf_loss': 176.52407836914062, 'total_loss': 124.17696380615234}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.927618026733398, 'policy_loss': 36.09560775756836, 'vf_loss': 165.635009765625, 'total_loss': 118.84384155273438}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.927174091339111, 'policy_loss': 17.834684371948242, 'vf_loss': 44.141510009765625, 'total_loss': 39.8361701965332}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9213738441467285, 'policy_loss': 28.727767944335938, 'vf_loss': 122.98715209960938, 'total_loss': 90.15213012695312}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.92186975479126, 'policy_loss': 36.89228057861328, 'vf_loss': 172.4123992919922, 'total_loss': 123.02925872802734}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.920482635498047, 'policy_loss': 17.588424682617188, 'vf_loss': 47.52696990966797, 'total_loss': 41.2827033996582}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.92960786819458, 'policy_loss': 34.5214958190918, 'vf_loss': 157.27882385253906, 'total_loss': 113.09160614013672}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.924274444580078, 'policy_loss': 25.21135711669922, 'vf_loss': 93.83858489990234, 'total_loss': 72.0614013671875}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.929357528686523, 'policy_loss': 35.605690002441406, 'vf_loss': 171.82008361816406, 'total_loss': 121.44644165039062}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.931119441986084, 'policy_loss': 37.09035873413086, 'vf_loss': 182.2008819580078, 'total_loss': 128.12149047851562}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9283552169799805, 'policy_loss': 19.62646484375, 'vf_loss': 60.56913375854492, 'total_loss': 49.84175109863281}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.909179210662842, 'policy_loss': 36.639190673828125, 'vf_loss': 164.74765014648438, 'total_loss': 118.94392395019531}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.920259952545166, 'policy_loss': 30.0522518157959, 'vf_loss': 130.96502685546875, 'total_loss': 95.46556091308594}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.907971382141113, 'policy_loss': 30.070083618164062, 'vf_loss': 114.31271362304688, 'total_loss': 87.15736389160156}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.92745304107666, 'policy_loss': 35.7443962097168, 'vf_loss': 169.8169403076172, 'total_loss': 120.58358764648438}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930917739868164, 'policy_loss': 35.94640350341797, 'vf_loss': 169.60098266601562, 'total_loss': 120.6775894165039}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.927764415740967, 'policy_loss': 24.41619873046875, 'vf_loss': 93.75196838378906, 'total_loss': 71.22290802001953}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930449962615967, 'policy_loss': 35.9752197265625, 'vf_loss': 174.3731689453125, 'total_loss': 123.09249877929688}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.922533988952637, 'policy_loss': 37.39665603637695, 'vf_loss': 172.4518585205078, 'total_loss': 123.55335998535156}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.924742221832275, 'policy_loss': 35.518714904785156, 'vf_loss': 168.6114959716797, 'total_loss': 119.75521850585938}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930129528045654, 'policy_loss': 36.547489166259766, 'vf_loss': 175.70928955078125, 'total_loss': 124.33283233642578}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.928244113922119, 'policy_loss': 35.917205810546875, 'vf_loss': 169.3013458251953, 'total_loss': 120.49859619140625}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.929443359375, 'policy_loss': 18.008066177368164, 'vf_loss': 48.06013870239258, 'total_loss': 41.968841552734375}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.923103332519531, 'policy_loss': 16.624832153320312, 'vf_loss': 38.506446838378906, 'total_loss': 35.80882263183594}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9234490394592285, 'policy_loss': 36.283966064453125, 'vf_loss': 163.36312866210938, 'total_loss': 117.89629364013672}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930476188659668, 'policy_loss': 29.54561996459961, 'vf_loss': 129.94482421875, 'total_loss': 94.44873046875}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.928630828857422, 'policy_loss': 37.34190368652344, 'vf_loss': 177.21995544433594, 'total_loss': 125.88259887695312}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.92788028717041, 'policy_loss': 18.453533172607422, 'vf_loss': 58.18259811401367, 'total_loss': 47.47555160522461}
[33m(raylet)[39m [2022-09-30 17:02:54,427 E 42920 42952] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2022-09-30_17-02-20_910330_42836 is over 95% full, available space: 4572209152; capacity: 125844406272. Object creation will fail if spilling is required.
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.927145004272461, 'policy_loss': 19.284168243408203, 'vf_loss': 60.25819396972656, 'total_loss': 49.343994140625}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.928404331207275, 'policy_loss': 37.2543830871582, 'vf_loss': 186.59133911132812, 'total_loss': 130.4807586669922}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.931168079376221, 'policy_loss': 35.67445373535156, 'vf_loss': 167.62069702148438, 'total_loss': 119.41548919677734}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.92866325378418, 'policy_loss': 35.267494201660156, 'vf_loss': 158.3267364501953, 'total_loss': 114.361572265625}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.928603172302246, 'policy_loss': 19.825332641601562, 'vf_loss': 64.76329040527344, 'total_loss': 52.137691497802734}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.924500465393066, 'policy_loss': 36.150230407714844, 'vf_loss': 162.61880493164062, 'total_loss': 117.39038848876953}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.925428867340088, 'policy_loss': 36.44055938720703, 'vf_loss': 187.504638671875, 'total_loss': 130.1236114501953}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.929109573364258, 'policy_loss': 22.425891876220703, 'vf_loss': 92.29421997070312, 'total_loss': 68.50370788574219}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.921113967895508, 'policy_loss': 35.47707748413086, 'vf_loss': 154.0629425048828, 'total_loss': 112.4393310546875}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.918979167938232, 'policy_loss': 17.49407958984375, 'vf_loss': 45.40515899658203, 'total_loss': 40.12746810913086}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.91963005065918, 'policy_loss': 19.700664520263672, 'vf_loss': 67.24092102050781, 'total_loss': 53.251930236816406}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.929631233215332, 'policy_loss': 33.529850006103516, 'vf_loss': 151.43101501464844, 'total_loss': 109.17606353759766}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.927687168121338, 'policy_loss': 28.294893264770508, 'vf_loss': 123.26994323730469, 'total_loss': 89.86058807373047}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.929637432098389, 'policy_loss': 12.292915344238281, 'vf_loss': 26.171722412109375, 'total_loss': 25.309480667114258}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9292683601379395, 'policy_loss': 13.268924713134766, 'vf_loss': 31.483198165893555, 'total_loss': 28.941232681274414}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.922411918640137, 'policy_loss': 36.287261962890625, 'vf_loss': 169.16957092285156, 'total_loss': 120.80282592773438}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.924996376037598, 'policy_loss': 33.79924392700195, 'vf_loss': 159.72264099121094, 'total_loss': 113.59131622314453}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.922215938568115, 'policy_loss': 26.203880310058594, 'vf_loss': 113.95315551757812, 'total_loss': 83.11123657226562}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.919304847717285, 'policy_loss': 15.502695083618164, 'vf_loss': 44.48423767089844, 'total_loss': 37.67561721801758}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.905387878417969, 'policy_loss': 38.53046417236328, 'vf_loss': 181.12933349609375, 'total_loss': 129.02606201171875}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.905908584594727, 'policy_loss': 35.999916076660156, 'vf_loss': 170.97711181640625, 'total_loss': 121.4194107055664}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.905649185180664, 'policy_loss': 35.44120788574219, 'vf_loss': 169.12448120117188, 'total_loss': 119.93439483642578}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.897701263427734, 'policy_loss': 37.91668701171875, 'vf_loss': 164.51028442382812, 'total_loss': 120.10285186767578}
No gradient information.s
training epoch 1 89 62.0 19.93258426966292
Main trainign step 2
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.905196666717529, 'policy_loss': 32.50226974487305, 'vf_loss': 170.2176055908203, 'total_loss': 117.5420150756836}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.906549453735352, 'policy_loss': 10.359169960021973, 'vf_loss': 27.23594856262207, 'total_loss': 23.908079147338867}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.90244197845459, 'policy_loss': 17.08525276184082, 'vf_loss': 54.204063415527344, 'total_loss': 44.118263244628906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.883551597595215, 'policy_loss': 38.399879455566406, 'vf_loss': 171.27066040039062, 'total_loss': 123.96637725830078}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.888200283050537, 'policy_loss': 36.28087615966797, 'vf_loss': 166.64080810546875, 'total_loss': 119.53239440917969}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.883105278015137, 'policy_loss': 36.89036178588867, 'vf_loss': 172.9297637939453, 'total_loss': 123.28640747070312}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.889506816864014, 'policy_loss': 34.53255844116211, 'vf_loss': 160.9183807373047, 'total_loss': 114.9228515625}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.900712490081787, 'policy_loss': 17.86778450012207, 'vf_loss': 77.91114044189453, 'total_loss': 56.75434875488281}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.886000633239746, 'policy_loss': 24.266132354736328, 'vf_loss': 96.7948226928711, 'total_loss': 72.59468078613281}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.8793816566467285, 'policy_loss': 34.84590148925781, 'vf_loss': 168.89015197753906, 'total_loss': 119.22218322753906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.8954057693481445, 'policy_loss': 16.658071517944336, 'vf_loss': 68.9981689453125, 'total_loss': 51.08820343017578}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.86912727355957, 'policy_loss': 37.21323776245117, 'vf_loss': 171.56936645507812, 'total_loss': 122.92922973632812}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.864943981170654, 'policy_loss': 36.20684051513672, 'vf_loss': 170.0770721435547, 'total_loss': 121.17672729492188}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.880942344665527, 'policy_loss': 31.9693603515625, 'vf_loss': 155.54351806640625, 'total_loss': 109.67230987548828}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.870086669921875, 'policy_loss': 16.236722946166992, 'vf_loss': 64.1553726196289, 'total_loss': 48.245704650878906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.851768970489502, 'policy_loss': 37.296478271484375, 'vf_loss': 170.02645874023438, 'total_loss': 122.2411880493164}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.863006591796875, 'policy_loss': 15.885018348693848, 'vf_loss': 63.250431060791016, 'total_loss': 47.44160461425781}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.8399810791015625, 'policy_loss': 35.69032669067383, 'vf_loss': 168.19891357421875, 'total_loss': 119.72138214111328}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.834815502166748, 'policy_loss': 39.06098175048828, 'vf_loss': 169.43984985351562, 'total_loss': 123.71255493164062}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.834376335144043, 'policy_loss': 31.06061553955078, 'vf_loss': 165.7418212890625, 'total_loss': 113.8631820678711}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.837216377258301, 'policy_loss': 38.28152084350586, 'vf_loss': 165.06088256835938, 'total_loss': 120.74359130859375}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.839022159576416, 'policy_loss': 9.949592590332031, 'vf_loss': 41.56814193725586, 'total_loss': 30.665273666381836}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 29.06195640563965, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.831902980804443, 'policy_loss': 8.4057035446167, 'vf_loss': 21.15029525756836, 'total_loss': 18.91253089904785}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.820266246795654, 'policy_loss': 35.86537551879883, 'vf_loss': 166.141357421875, 'total_loss': 118.86785125732422}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 25.823942184448242, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.838404655456543, 'policy_loss': 8.208549499511719, 'vf_loss': 21.169952392578125, 'total_loss': 18.725141525268555}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.8240437507629395, 'policy_loss': 31.54608154296875, 'vf_loss': 156.26681518554688, 'total_loss': 109.61125183105469}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.830656051635742, 'policy_loss': 14.421218872070312, 'vf_loss': 64.746337890625, 'total_loss': 46.72608184814453}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.817834377288818, 'policy_loss': 36.36825180053711, 'vf_loss': 164.04820251464844, 'total_loss': 118.32417297363281}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.812617778778076, 'policy_loss': 16.22563934326172, 'vf_loss': 67.78411865234375, 'total_loss': 50.0495719909668}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.80915641784668, 'policy_loss': 36.335289001464844, 'vf_loss': 161.74612426757812, 'total_loss': 117.1402587890625}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.8201799392700195, 'policy_loss': 34.02039337158203, 'vf_loss': 166.8614044189453, 'total_loss': 117.38289642333984}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.817866325378418, 'policy_loss': 20.112247467041016, 'vf_loss': 91.7007827758789, 'total_loss': 65.89446258544922}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.824819087982178, 'policy_loss': 8.949851036071777, 'vf_loss': 23.500051498413086, 'total_loss': 20.631628036499023}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.817266941070557, 'policy_loss': 38.86316680908203, 'vf_loss': 173.45205688476562, 'total_loss': 125.5210189819336}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.825737953186035, 'policy_loss': 36.483482360839844, 'vf_loss': 157.66416931152344, 'total_loss': 115.24730682373047}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.838784694671631, 'policy_loss': 31.31365203857422, 'vf_loss': 170.11785888671875, 'total_loss': 116.30419158935547}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.8689727783203125, 'policy_loss': 30.533105850219727, 'vf_loss': 144.7145233154297, 'total_loss': 102.8216781616211}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.857132911682129, 'policy_loss': 25.122272491455078, 'vf_loss': 122.59095764160156, 'total_loss': 86.34918212890625}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.849993705749512, 'policy_loss': 14.8018798828125, 'vf_loss': 68.7528076171875, 'total_loss': 49.10978317260742}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.854894161224365, 'policy_loss': 34.80133056640625, 'vf_loss': 160.21559143066406, 'total_loss': 114.840576171875}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.842110633850098, 'policy_loss': 27.251934051513672, 'vf_loss': 130.19041442871094, 'total_loss': 92.27871704101562}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.839646816253662, 'policy_loss': 37.55760955810547, 'vf_loss': 169.4919891357422, 'total_loss': 122.2352066040039}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.84149169921875, 'policy_loss': 37.31292724609375, 'vf_loss': 163.277099609375, 'total_loss': 118.88306427001953}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 24.950775146484375, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.8533244132995605, 'policy_loss': 5.861353874206543, 'vf_loss': 30.640357971191406, 'total_loss': 21.113000869750977}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.852818012237549, 'policy_loss': 10.253454208374023, 'vf_loss': 28.659868240356445, 'total_loss': 24.514862060546875}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.855698108673096, 'policy_loss': 33.12348556518555, 'vf_loss': 157.86578369140625, 'total_loss': 111.98782348632812}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.861418724060059, 'policy_loss': 12.961222648620605, 'vf_loss': 48.44541549682617, 'total_loss': 37.11531448364258}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.861546516418457, 'policy_loss': 32.83383560180664, 'vf_loss': 187.77935791015625, 'total_loss': 126.65489196777344}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.863449573516846, 'policy_loss': 34.36891555786133, 'vf_loss': 159.0450897216797, 'total_loss': 113.82283020019531}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.869617938995361, 'policy_loss': 30.873241424560547, 'vf_loss': 147.46255493164062, 'total_loss': 104.53582763671875}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 20.972543716430664, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.870551109313965, 'policy_loss': 5.7757248878479, 'vf_loss': 17.416461944580078, 'total_loss': 14.415249824523926}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 19.350019454956055, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.867457866668701, 'policy_loss': 5.485828399658203, 'vf_loss': 19.066539764404297, 'total_loss': 14.950423240661621}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.8666276931762695, 'policy_loss': 39.964054107666016, 'vf_loss': 169.54928588867188, 'total_loss': 124.67002868652344}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 10.025567054748535, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.86841344833374, 'policy_loss': 2.962130546569824, 'vf_loss': 21.248056411743164, 'total_loss': 13.517474174499512}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.874379634857178, 'policy_loss': 34.611148834228516, 'vf_loss': 162.31051635742188, 'total_loss': 115.69766235351562}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 9.246651649475098, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.881167411804199, 'policy_loss': 2.0820140838623047, 'vf_loss': 19.139020919799805, 'total_loss': 11.58271312713623}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.877384185791016, 'policy_loss': 36.51293182373047, 'vf_loss': 161.89505004882812, 'total_loss': 117.39168548583984}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.882187366485596, 'policy_loss': 35.14736557006836, 'vf_loss': 168.087158203125, 'total_loss': 119.12212371826172}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.89128303527832, 'policy_loss': 2.8920953273773193, 'vf_loss': 21.278711318969727, 'total_loss': 13.462538719177246}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 28.368410110473633, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.891210079193115, 'policy_loss': 8.425402641296387, 'vf_loss': 38.771522521972656, 'total_loss': 27.742252349853516}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.899061679840088, 'policy_loss': 35.12290573120117, 'vf_loss': 154.40509033203125, 'total_loss': 112.25646209716797}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.906254291534424, 'policy_loss': 34.41983413696289, 'vf_loss': 170.61764526367188, 'total_loss': 119.65959167480469}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.912295818328857, 'policy_loss': 34.88199996948242, 'vf_loss': 159.52203369140625, 'total_loss': 114.57389831542969}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.910975456237793, 'policy_loss': 37.2153434753418, 'vf_loss': 164.8777313232422, 'total_loss': 119.58509826660156}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.918639659881592, 'policy_loss': 33.424007415771484, 'vf_loss': 165.75076293945312, 'total_loss': 116.23020935058594}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9216413497924805, 'policy_loss': 35.116939544677734, 'vf_loss': 160.32229614257812, 'total_loss': 115.20887756347656}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 27.804540634155273, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.923398971557617, 'policy_loss': 5.247136116027832, 'vf_loss': 38.74658966064453, 'total_loss': 24.55119514465332}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.924295425415039, 'policy_loss': 34.447669982910156, 'vf_loss': 160.1953887939453, 'total_loss': 114.47611999511719}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9274797439575195, 'policy_loss': 0.908324122428894, 'vf_loss': 19.82793426513672, 'total_loss': 10.753016471862793}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9295549392700195, 'policy_loss': 34.3195686340332, 'vf_loss': 161.08660888671875, 'total_loss': 114.79357147216797}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 18.771148681640625, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930450916290283, 'policy_loss': 1.7267811298370361, 'vf_loss': 25.85320472717285, 'total_loss': 14.584078788757324}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930917739868164, 'policy_loss': 35.192134857177734, 'vf_loss': 162.25454711914062, 'total_loss': 116.25010681152344}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.931396007537842, 'policy_loss': 34.983455657958984, 'vf_loss': 160.1670379638672, 'total_loss': 114.9976577758789}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 18.24724006652832, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9307122230529785, 'policy_loss': 0.7297837734222412, 'vf_loss': 18.277118682861328, 'total_loss': 9.799036026000977}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9311981201171875, 'policy_loss': 34.95552444458008, 'vf_loss': 158.07322692871094, 'total_loss': 113.9228286743164}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 26.97505760192871, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.931270599365234, 'policy_loss': 3.575119972229004, 'vf_loss': 36.21963882446289, 'total_loss': 21.61562728881836}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.931427001953125, 'policy_loss': 35.24659729003906, 'vf_loss': 162.17413330078125, 'total_loss': 116.26435089111328}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.931425094604492, 'policy_loss': 35.288475036621094, 'vf_loss': 163.0189971923828, 'total_loss': 116.7286605834961}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 19.02997589111328, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930992126464844, 'policy_loss': 3.0949692726135254, 'vf_loss': 35.11811065673828, 'total_loss': 20.584714889526367}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.931374549865723, 'policy_loss': 35.164695739746094, 'vf_loss': 163.28892517089844, 'total_loss': 116.7398452758789}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.931338787078857, 'policy_loss': 34.72908020019531, 'vf_loss': 160.01889038085938, 'total_loss': 114.6692123413086}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930902481079102, 'policy_loss': 34.81791687011719, 'vf_loss': 158.3819122314453, 'total_loss': 113.93956756591797}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.929886817932129, 'policy_loss': 35.77033615112305, 'vf_loss': 164.96957397460938, 'total_loss': 118.18582916259766}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930288314819336, 'policy_loss': 35.17490768432617, 'vf_loss': 164.983642578125, 'total_loss': 117.59742736816406}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930636882781982, 'policy_loss': 34.1911506652832, 'vf_loss': 154.20919799804688, 'total_loss': 111.2264404296875}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930143356323242, 'policy_loss': 33.445552825927734, 'vf_loss': 146.88345336914062, 'total_loss': 106.81798553466797}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.929206848144531, 'policy_loss': 17.46184730529785, 'vf_loss': 88.85220336914062, 'total_loss': 61.81865692138672}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 11.717491149902344, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.928775787353516, 'policy_loss': -0.27529382705688477, 'vf_loss': 28.890296936035156, 'total_loss': 14.100567817687988}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.92800760269165, 'policy_loss': 33.52848815917969, 'vf_loss': 150.52597045898438, 'total_loss': 108.7221908569336}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 25.471723556518555, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.925442695617676, 'policy_loss': 6.332607746124268, 'vf_loss': 55.75107192993164, 'total_loss': 34.13888931274414}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.920997619628906, 'policy_loss': 35.939876556396484, 'vf_loss': 167.07029724121094, 'total_loss': 119.40581512451172}
No gradient information.s
training epoch 2 100 72.0 22.55
Main trainign step 3
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.922102928161621, 'policy_loss': 32.786014556884766, 'vf_loss': 148.33836364746094, 'total_loss': 106.88597869873047}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 19.874370574951172, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.922797203063965, 'policy_loss': 0.6265829801559448, 'vf_loss': 23.428653717041016, 'total_loss': 12.271681785583496}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.912674427032471, 'policy_loss': 31.84440803527832, 'vf_loss': 140.2259063720703, 'total_loss': 101.88822937011719}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 10.79974365234375, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.905016899108887, 'policy_loss': -1.5133404731750488, 'vf_loss': 23.435081481933594, 'total_loss': 10.135150909423828}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.902859210968018, 'policy_loss': 34.885459899902344, 'vf_loss': 163.6621856689453, 'total_loss': 116.64752197265625}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.8997883796691895, 'policy_loss': 34.03821563720703, 'vf_loss': 157.4993896484375, 'total_loss': 112.71891021728516}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.898613452911377, 'policy_loss': 34.248416900634766, 'vf_loss': 157.11398315429688, 'total_loss': 112.73641967773438}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.891030788421631, 'policy_loss': 8.322677612304688, 'vf_loss': 66.70106506347656, 'total_loss': 41.60430145263672}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.889253616333008, 'policy_loss': 3.7241687774658203, 'vf_loss': 43.59159851074219, 'total_loss': 25.451074600219727}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.869557857513428, 'policy_loss': 7.276125431060791, 'vf_loss': 51.33255386352539, 'total_loss': 32.87370681762695}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 20.103199005126953, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.857143402099609, 'policy_loss': -2.7854456901550293, 'vf_loss': 21.915578842163086, 'total_loss': 8.10377311706543}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.833892822265625, 'policy_loss': 33.307559967041016, 'vf_loss': 136.01300048828125, 'total_loss': 101.24571990966797}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.832830905914307, 'policy_loss': 14.220174789428711, 'vf_loss': 72.26325225830078, 'total_loss': 50.283470153808594}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 25.208595275878906, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.811191082000732, 'policy_loss': -0.21377193927764893, 'vf_loss': 43.62419509887695, 'total_loss': 21.530214309692383}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 17.237401962280273, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.83657169342041, 'policy_loss': -0.9280860424041748, 'vf_loss': 30.607900619506836, 'total_loss': 14.30749797821045}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.789324760437012, 'policy_loss': 37.753082275390625, 'vf_loss': 157.7924346923828, 'total_loss': 116.58140563964844}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.799447059631348, 'policy_loss': 25.96906089782715, 'vf_loss': 141.558349609375, 'total_loss': 96.68024444580078}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 22.10715675354004, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.832657814025879, 'policy_loss': -2.61392879486084, 'vf_loss': 19.46221351623535, 'total_loss': 7.048851490020752}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 25.170921325683594, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.806257247924805, 'policy_loss': -2.268674850463867, 'vf_loss': 17.997364044189453, 'total_loss': 6.661944389343262}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.785584449768066, 'policy_loss': 28.18667221069336, 'vf_loss': 140.21255493164062, 'total_loss': 98.22509765625}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.76603889465332, 'policy_loss': 12.986591339111328, 'vf_loss': 71.7778549194336, 'total_loss': 48.807857513427734}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.742756366729736, 'policy_loss': 32.227569580078125, 'vf_loss': 139.37518310546875, 'total_loss': 101.84773254394531}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.782925605773926, 'policy_loss': 3.0113275051116943, 'vf_loss': 45.36747360229492, 'total_loss': 25.627235412597656}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.748483657836914, 'policy_loss': 5.1454362869262695, 'vf_loss': 53.67057418823242, 'total_loss': 31.913240432739258}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.702914714813232, 'policy_loss': 33.69504165649414, 'vf_loss': 138.8037109375, 'total_loss': 103.02986145019531}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 29.72484016418457, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.758471488952637, 'policy_loss': 1.4342390298843384, 'vf_loss': 31.063888549804688, 'total_loss': 16.898597717285156}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.687750816345215, 'policy_loss': 35.94609069824219, 'vf_loss': 137.26943969726562, 'total_loss': 104.51393127441406}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.726517200469971, 'policy_loss': 3.6406898498535156, 'vf_loss': 53.08097457885742, 'total_loss': 30.11391258239746}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.706578731536865, 'policy_loss': -3.834660053253174, 'vf_loss': 29.726076126098633, 'total_loss': 10.961311340332031}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.6937150955200195, 'policy_loss': -5.003340721130371, 'vf_loss': 24.260175704956055, 'total_loss': 7.059810161590576}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.574714183807373, 'policy_loss': 31.712003707885742, 'vf_loss': 156.18365478515625, 'total_loss': 109.73808288574219}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.65790319442749, 'policy_loss': 1.4094090461730957, 'vf_loss': 21.763957977294922, 'total_loss': 12.224808692932129}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.581553936004639, 'policy_loss': 26.030935287475586, 'vf_loss': 119.42234802246094, 'total_loss': 85.6762924194336}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.657198905944824, 'policy_loss': 2.784865617752075, 'vf_loss': 30.403303146362305, 'total_loss': 17.919944763183594}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.646607398986816, 'policy_loss': 4.163874626159668, 'vf_loss': 29.821380615234375, 'total_loss': 19.008100509643555}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.558943748474121, 'policy_loss': -4.941930294036865, 'vf_loss': 22.378767013549805, 'total_loss': 6.181863784790039}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.593204021453857, 'policy_loss': 0.8208674192428589, 'vf_loss': 10.54520320892334, 'total_loss': 6.0275373458862305}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.455933094024658, 'policy_loss': 40.260990142822266, 'vf_loss': 173.29434204101562, 'total_loss': 126.84359741210938}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.3386640548706055, 'policy_loss': 6.607237339019775, 'vf_loss': 62.544456481933594, 'total_loss': 37.81608200073242}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.622176647186279, 'policy_loss': 3.0979323387145996, 'vf_loss': 11.665377616882324, 'total_loss': 8.864398956298828}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.420459270477295, 'policy_loss': 17.468339920043945, 'vf_loss': 78.6182861328125, 'total_loss': 56.71327590942383}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.439196586608887, 'policy_loss': 6.050187587738037, 'vf_loss': 40.89909362792969, 'total_loss': 26.43534278869629}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.231838703155518, 'policy_loss': 33.64881134033203, 'vf_loss': 153.24855041503906, 'total_loss': 110.21076965332031}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.495243072509766, 'policy_loss': 20.3560791015625, 'vf_loss': 69.85509490966797, 'total_loss': 55.21867370605469}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.373534202575684, 'policy_loss': 19.401700973510742, 'vf_loss': 83.4962158203125, 'total_loss': 61.08607482910156}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.538332462310791, 'policy_loss': 1.3053317070007324, 'vf_loss': 29.67278289794922, 'total_loss': 16.076339721679688}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.563362121582031, 'policy_loss': 1.7154902219772339, 'vf_loss': 22.88261604309082, 'total_loss': 13.091164588928223}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 24.367204666137695, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.543573379516602, 'policy_loss': 3.2302656173706055, 'vf_loss': 7.490941047668457, 'total_loss': 6.910300254821777}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 20.865758895874023, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.4960618019104, 'policy_loss': -0.9433287978172302, 'vf_loss': 8.398787498474121, 'total_loss': 3.1911041736602783}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.121015548706055, 'policy_loss': 36.35116195678711, 'vf_loss': 130.96742248535156, 'total_loss': 101.7736587524414}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.4619550704956055, 'policy_loss': 32.8545036315918, 'vf_loss': 142.5546112060547, 'total_loss': 104.06718444824219}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.2975335121154785, 'policy_loss': 16.703025817871094, 'vf_loss': 80.64383697509766, 'total_loss': 56.96196746826172}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 25.832780838012695, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.438288688659668, 'policy_loss': 2.085418224334717, 'vf_loss': 19.274349212646484, 'total_loss': 11.658210754394531}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 22.950328826904297, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.541892051696777, 'policy_loss': 3.821120023727417, 'vf_loss': 10.649420738220215, 'total_loss': 9.080410957336426}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 25.519176483154297, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.426736831665039, 'policy_loss': 0.03228175640106201, 'vf_loss': 12.50473403930664, 'total_loss': 6.220381736755371}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.068077564239502, 'policy_loss': 50.251953125, 'vf_loss': 161.71128845214844, 'total_loss': 131.0469207763672}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.099322319030762, 'policy_loss': 29.70254898071289, 'vf_loss': 134.72142028808594, 'total_loss': 97.00226593017578}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.691017150878906, 'policy_loss': 6.3881049156188965, 'vf_loss': 13.411563873291016, 'total_loss': 13.0269775390625}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.368309020996094, 'policy_loss': 12.543437957763672, 'vf_loss': 31.194473266601562, 'total_loss': 28.07699203491211}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.47925329208374, 'policy_loss': 6.649968147277832, 'vf_loss': 16.859235763549805, 'total_loss': 15.014793395996094}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 20.355680465698242, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.562771320343018, 'policy_loss': 4.367757797241211, 'vf_loss': 12.62973403930664, 'total_loss': 10.616996765136719}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 21.143218994140625, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.578062057495117, 'policy_loss': -2.1577858924865723, 'vf_loss': 23.481220245361328, 'total_loss': 9.517044067382812}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.512795925140381, 'policy_loss': 28.49018096923828, 'vf_loss': 116.63566589355469, 'total_loss': 86.74288940429688}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.528617858886719, 'policy_loss': 7.680248260498047, 'vf_loss': 57.83784866333008, 'total_loss': 36.53388977050781}
[33m(raylet)[39m [2022-09-30 17:03:04,474 E 42920 42952] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2022-09-30_17-02-20_910330_42836 is over 95% full, available space: 4571807744; capacity: 125844406272. Object creation will fail if spilling is required.
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.52194881439209, 'policy_loss': 17.63223648071289, 'vf_loss': 67.73900604248047, 'total_loss': 51.436519622802734}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.336255073547363, 'policy_loss': 15.757536888122559, 'vf_loss': 78.27947235107422, 'total_loss': 54.83391189575195}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.364746570587158, 'policy_loss': -1.7247323989868164, 'vf_loss': 18.927242279052734, 'total_loss': 7.675241470336914}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 23.01630973815918, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.524620056152344, 'policy_loss': -0.4285781979560852, 'vf_loss': 8.736510276794434, 'total_loss': 3.8744308948516846}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.370388031005859, 'policy_loss': 9.793745040893555, 'vf_loss': 30.157052993774414, 'total_loss': 24.808568954467773}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.2889275550842285, 'policy_loss': 25.740259170532227, 'vf_loss': 84.42413330078125, 'total_loss': 67.88943481445312}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.183756351470947, 'policy_loss': 35.01912307739258, 'vf_loss': 138.47691345214844, 'total_loss': 104.19574737548828}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.504663467407227, 'policy_loss': 20.227624893188477, 'vf_loss': 71.93959045410156, 'total_loss': 56.13236999511719}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.378617763519287, 'policy_loss': 10.310938835144043, 'vf_loss': 38.09275436401367, 'total_loss': 29.293529510498047}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.4021477699279785, 'policy_loss': 26.394054412841797, 'vf_loss': 59.75541687011719, 'total_loss': 56.207740783691406}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.303631782531738, 'policy_loss': 1.5511412620544434, 'vf_loss': 59.635169982910156, 'total_loss': 31.30569076538086}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 22.927364349365234, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.691794395446777, 'policy_loss': 4.274887561798096, 'vf_loss': 12.753890037536621, 'total_loss': 10.584914207458496}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.5961737632751465, 'policy_loss': -2.7847728729248047, 'vf_loss': 20.08979606628418, 'total_loss': 7.1941633224487305}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 26.060914993286133, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.626400947570801, 'policy_loss': -5.757291793823242, 'vf_loss': 28.04610824584961, 'total_loss': 8.199498176574707}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.416824817657471, 'policy_loss': 36.86878967285156, 'vf_loss': 157.24002075195312, 'total_loss': 115.42462921142578}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.566625595092773, 'policy_loss': 14.902128219604492, 'vf_loss': 50.74824523925781, 'total_loss': 40.21058654785156}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.521124839782715, 'policy_loss': 29.334415435791016, 'vf_loss': 83.59467315673828, 'total_loss': 71.06654357910156}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.562087059020996, 'policy_loss': -16.652873992919922, 'vf_loss': 48.48645782470703, 'total_loss': 7.524734020233154}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.7247114181518555, 'policy_loss': 1.2771027088165283, 'vf_loss': 20.06707763671875, 'total_loss': 11.243393898010254}
No gradient information.s
> CALLBACK HAD NON-ZERO INFO {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.7247114181518555, 'policy_loss': 1.2771027088165283, 'vf_loss': 20.06707763671875, 'total_loss': 11.243393898010254}
training epoch 3 100 72.0 20.01
Main trainign step 4
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.766791343688965, 'policy_loss': 5.240180015563965, 'vf_loss': 12.86017894744873, 'total_loss': 11.602601051330566}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.63342809677124, 'policy_loss': 10.365677833557129, 'vf_loss': 44.31059646606445, 'total_loss': 32.45464324951172}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.6110639572143555, 'policy_loss': 28.956920623779297, 'vf_loss': 129.44189453125, 'total_loss': 93.61176300048828}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.680197238922119, 'policy_loss': 7.554244041442871, 'vf_loss': 58.10850524902344, 'total_loss': 36.54169464111328}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 24.59687614440918, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.696470737457275, 'policy_loss': -1.68678617477417, 'vf_loss': 26.446369171142578, 'total_loss': 11.469432830810547}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.479590892791748, 'policy_loss': 29.62381362915039, 'vf_loss': 138.0263214111328, 'total_loss': 98.57218170166016}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.575658798217773, 'policy_loss': 26.587528228759766, 'vf_loss': 100.66085052490234, 'total_loss': 76.8521957397461}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 12.41627311706543, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.7708353996276855, 'policy_loss': 1.5545181035995483, 'vf_loss': 19.40363121032715, 'total_loss': 11.18862533569336}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.712591171264648, 'policy_loss': -2.4946351051330566, 'vf_loss': 15.381900787353516, 'total_loss': 5.129189491271973}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.573556423187256, 'policy_loss': 18.67096710205078, 'vf_loss': 59.89271545410156, 'total_loss': 48.55158996582031}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 24.067516326904297, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.752803802490234, 'policy_loss': 0.4961234927177429, 'vf_loss': 23.75259780883789, 'total_loss': 12.30489444732666}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.6546478271484375, 'policy_loss': 4.766746520996094, 'vf_loss': 39.37164306640625, 'total_loss': 24.38602066040039}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.5490498542785645, 'policy_loss': 31.42390251159668, 'vf_loss': 143.42416381835938, 'total_loss': 103.07049560546875}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.7185378074646, 'policy_loss': -1.5513052940368652, 'vf_loss': 21.287986755371094, 'total_loss': 9.02550220489502}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.614039421081543, 'policy_loss': 39.60757064819336, 'vf_loss': 172.87696838378906, 'total_loss': 125.97991180419922}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.729367733001709, 'policy_loss': -5.185442924499512, 'vf_loss': 14.62399673461914, 'total_loss': 2.0592617988586426}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 18.329227447509766, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.756865501403809, 'policy_loss': -0.29754236340522766, 'vf_loss': 3.737074851989746, 'total_loss': 1.503426432609558}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.585425853729248, 'policy_loss': 35.55839157104492, 'vf_loss': 155.60903930664062, 'total_loss': 113.29705810546875}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.64948844909668, 'policy_loss': 19.445505142211914, 'vf_loss': 74.13478088378906, 'total_loss': 56.4463996887207}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.764089584350586, 'policy_loss': 1.6510248184204102, 'vf_loss': 25.302736282348633, 'total_loss': 14.23475170135498}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.641603469848633, 'policy_loss': 32.178531646728516, 'vf_loss': 145.16477966308594, 'total_loss': 104.69451141357422}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.815427303314209, 'policy_loss': 30.568368911743164, 'vf_loss': 150.15939331054688, 'total_loss': 105.57991027832031}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.819796085357666, 'policy_loss': 8.837051391601562, 'vf_loss': 10.334647178649902, 'total_loss': 13.936177253723145}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.70092248916626, 'policy_loss': 36.52101135253906, 'vf_loss': 154.7937469482422, 'total_loss': 113.85087585449219}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.726340293884277, 'policy_loss': 32.86054611206055, 'vf_loss': 152.56735229492188, 'total_loss': 109.07696533203125}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.741257190704346, 'policy_loss': -16.531150817871094, 'vf_loss': 68.00883483886719, 'total_loss': 17.405853271484375}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.794156551361084, 'policy_loss': 28.55059242248535, 'vf_loss': 110.14262390136719, 'total_loss': 83.55396270751953}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.849272727966309, 'policy_loss': -6.5960235595703125, 'vf_loss': 25.070714950561523, 'total_loss': 5.870841026306152}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.78628396987915, 'policy_loss': 35.33075714111328, 'vf_loss': 155.6207733154297, 'total_loss': 113.07328033447266}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.817285537719727, 'policy_loss': 29.99993896484375, 'vf_loss': 124.97242736816406, 'total_loss': 92.41797637939453}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 23.57196807861328, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.881955623626709, 'policy_loss': 4.8819661140441895, 'vf_loss': 23.434242248535156, 'total_loss': 16.5302677154541}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.835564613342285, 'policy_loss': 15.789973258972168, 'vf_loss': 38.79678726196289, 'total_loss': 35.12001037597656}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.846498489379883, 'policy_loss': 29.813552856445312, 'vf_loss': 137.94464111328125, 'total_loss': 98.7174072265625}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.8705010414123535, 'policy_loss': 37.20283508300781, 'vf_loss': 175.50088500976562, 'total_loss': 124.88457489013672}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.877718925476074, 'policy_loss': 13.785684585571289, 'vf_loss': 41.71528244018555, 'total_loss': 34.574546813964844}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.919507026672363, 'policy_loss': 45.614959716796875, 'vf_loss': 270.2980651855469, 'total_loss': 180.69479370117188}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.908422470092773, 'policy_loss': -7.72041654586792, 'vf_loss': 39.4600830078125, 'total_loss': 11.940540313720703}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.894869804382324, 'policy_loss': 36.089115142822266, 'vf_loss': 166.50552368164062, 'total_loss': 119.2729263305664}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.891197204589844, 'policy_loss': -19.050106048583984, 'vf_loss': 71.63008117675781, 'total_loss': 16.696022033691406}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.90184211730957, 'policy_loss': 34.10295867919922, 'vf_loss': 153.2583770751953, 'total_loss': 110.66313171386719}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.907064914703369, 'policy_loss': 33.169219970703125, 'vf_loss': 142.766845703125, 'total_loss': 104.48357391357422}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.907175064086914, 'policy_loss': 30.71026611328125, 'vf_loss': 123.1861801147461, 'total_loss': 92.23429107666016}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.925199508666992, 'policy_loss': 22.325286865234375, 'vf_loss': 84.47301483154297, 'total_loss': 64.49254608154297}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 25.5594425201416, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.917917251586914, 'policy_loss': 4.688455104827881, 'vf_loss': 29.562715530395508, 'total_loss': 19.400632858276367}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.861305236816406, 'policy_loss': 9.338692665100098, 'vf_loss': 14.636764526367188, 'total_loss': 16.58846092224121}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.911722183227539, 'policy_loss': -4.588550567626953, 'vf_loss': 20.070903778076172, 'total_loss': 5.377784252166748}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.92672061920166, 'policy_loss': 36.635013580322266, 'vf_loss': 179.2177734375, 'total_loss': 126.17462921142578}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.922173500061035, 'policy_loss': 34.482303619384766, 'vf_loss': 154.27830505371094, 'total_loss': 111.55223846435547}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.924205780029297, 'policy_loss': 33.72938537597656, 'vf_loss': 152.9596405029297, 'total_loss': 110.13996124267578}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.924305438995361, 'policy_loss': -20.170562744140625, 'vf_loss': 78.29276275634766, 'total_loss': 18.90657615661621}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9281415939331055, 'policy_loss': 25.28440284729004, 'vf_loss': 100.12677001953125, 'total_loss': 75.27850341796875}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.917349815368652, 'policy_loss': -8.081550598144531, 'vf_loss': 36.78617858886719, 'total_loss': 10.242364883422852}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.912229537963867, 'policy_loss': 21.78916358947754, 'vf_loss': 63.48832321166992, 'total_loss': 53.464202880859375}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.919824123382568, 'policy_loss': 17.9981632232666, 'vf_loss': 102.39671325683594, 'total_loss': 69.1273193359375}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.92382287979126, 'policy_loss': 26.124027252197266, 'vf_loss': 97.04927825927734, 'total_loss': 74.57942962646484}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.90733003616333, 'policy_loss': -2.4965577125549316, 'vf_loss': 26.545137405395508, 'total_loss': 10.706936836242676}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.918966770172119, 'policy_loss': 35.522705078125, 'vf_loss': 154.31961059570312, 'total_loss': 112.61331939697266}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 15.61447525024414, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.924200534820557, 'policy_loss': 3.5157270431518555, 'vf_loss': 51.051212310791016, 'total_loss': 28.972089767456055}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.8707990646362305, 'policy_loss': 5.883209228515625, 'vf_loss': 8.14508056640625, 'total_loss': 9.887041091918945}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.922264099121094, 'policy_loss': 33.63249588012695, 'vf_loss': 185.02597045898438, 'total_loss': 126.07625579833984}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9260334968566895, 'policy_loss': 13.896952629089355, 'vf_loss': 96.11878204345703, 'total_loss': 61.8870849609375}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.924249172210693, 'policy_loss': 31.350509643554688, 'vf_loss': 135.80833435058594, 'total_loss': 99.18543243408203}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.900801658630371, 'policy_loss': 2.1037654876708984, 'vf_loss': 28.37594985961914, 'total_loss': 16.222732543945312}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 21.06377410888672, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.885628700256348, 'policy_loss': -2.1196558475494385, 'vf_loss': 19.339021682739258, 'total_loss': 7.480998992919922}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.927468299865723, 'policy_loss': 33.30812454223633, 'vf_loss': 149.37367248535156, 'total_loss': 107.92568969726562}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.928098678588867, 'policy_loss': 34.2310791015625, 'vf_loss': 148.99317932128906, 'total_loss': 108.65838623046875}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9266815185546875, 'policy_loss': 35.88262176513672, 'vf_loss': 165.75241088867188, 'total_loss': 118.68955993652344}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9235358238220215, 'policy_loss': -21.224628448486328, 'vf_loss': 85.75581359863281, 'total_loss': 21.584043502807617}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.925459861755371, 'policy_loss': 34.956153869628906, 'vf_loss': 153.32974243164062, 'total_loss': 111.55177307128906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.926816940307617, 'policy_loss': -19.751693725585938, 'vf_loss': 92.33314514160156, 'total_loss': 26.345611572265625}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.911741256713867, 'policy_loss': 11.122371673583984, 'vf_loss': 25.502403259277344, 'total_loss': 23.80445671081543}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.8726396560668945, 'policy_loss': 11.890216827392578, 'vf_loss': 40.35195541381836, 'total_loss': 31.997467041015625}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.927446365356445, 'policy_loss': 14.350666046142578, 'vf_loss': 45.51386260986328, 'total_loss': 37.03832244873047}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 24.46735382080078, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.864439487457275, 'policy_loss': 2.6253817081451416, 'vf_loss': 12.044180870056152, 'total_loss': 8.578827857971191}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.898061275482178, 'policy_loss': 0.8976861238479614, 'vf_loss': 39.30974578857422, 'total_loss': 20.483577728271484}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.925360679626465, 'policy_loss': 34.166099548339844, 'vf_loss': 151.1177215576172, 'total_loss': 109.65570831298828}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.927592754364014, 'policy_loss': 3.0668911933898926, 'vf_loss': 126.26002502441406, 'total_loss': 66.12763214111328}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.892210006713867, 'policy_loss': -8.968579292297363, 'vf_loss': 17.676454544067383, 'total_loss': -0.19927412271499634}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.927473068237305, 'policy_loss': 31.35348129272461, 'vf_loss': 138.688720703125, 'total_loss': 100.62857055664062}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 27.38239097595215, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.854405403137207, 'policy_loss': 1.2190561294555664, 'vf_loss': 8.452709197998047, 'total_loss': 5.376866817474365}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.924833297729492, 'policy_loss': 33.210472106933594, 'vf_loss': 147.0624542236328, 'total_loss': 106.67244720458984}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.926760673522949, 'policy_loss': 35.186222076416016, 'vf_loss': 171.23260498046875, 'total_loss': 120.7332534790039}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.928071022033691, 'policy_loss': 15.960638999938965, 'vf_loss': 57.7220344543457, 'total_loss': 44.75237274169922}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.909192085266113, 'policy_loss': 5.319572925567627, 'vf_loss': 45.99249267578125, 'total_loss': 28.246726989746094}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.92796516418457, 'policy_loss': 38.78048324584961, 'vf_loss': 203.1608123779297, 'total_loss': 140.29161071777344}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.913642883300781, 'policy_loss': -13.82727336883545, 'vf_loss': 39.95064163208008, 'total_loss': 6.078910827636719}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9236016273498535, 'policy_loss': 33.94273376464844, 'vf_loss': 149.05284118652344, 'total_loss': 108.39991760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.922855377197266, 'policy_loss': 35.103919982910156, 'vf_loss': 149.41319274902344, 'total_loss': 109.74128723144531}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.927796840667725, 'policy_loss': 11.11223030090332, 'vf_loss': 123.66268920898438, 'total_loss': 72.87429809570312}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.929167747497559, 'policy_loss': 28.380189895629883, 'vf_loss': 116.9193115234375, 'total_loss': 86.77055358886719}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.906267166137695, 'policy_loss': 22.917579650878906, 'vf_loss': 71.8345718383789, 'total_loss': 58.765804290771484}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.912277698516846, 'policy_loss': 9.531020164489746, 'vf_loss': 75.03079223632812, 'total_loss': 46.977294921875}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.931406021118164, 'policy_loss': 33.88311767578125, 'vf_loss': 152.65570068359375, 'total_loss': 110.14165496826172}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930619239807129, 'policy_loss': -16.21599006652832, 'vf_loss': 111.3252944946289, 'total_loss': 39.377349853515625}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.926711082458496, 'policy_loss': 26.329225540161133, 'vf_loss': 99.59294128417969, 'total_loss': 76.05642700195312}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.92430305480957, 'policy_loss': 34.19252014160156, 'vf_loss': 172.71177673339844, 'total_loss': 120.47916412353516}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9135613441467285, 'policy_loss': 33.904937744140625, 'vf_loss': 139.428466796875, 'total_loss': 103.55003356933594}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.908984184265137, 'policy_loss': 16.68101692199707, 'vf_loss': 133.4723663330078, 'total_loss': 83.34810638427734}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.879485607147217, 'policy_loss': -13.347286224365234, 'vf_loss': 42.48018264770508, 'total_loss': 7.824010372161865}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.922951698303223, 'policy_loss': 34.301597595214844, 'vf_loss': 150.2400360107422, 'total_loss': 109.35238647460938}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.919891357421875, 'policy_loss': 33.29700469970703, 'vf_loss': 133.56471252441406, 'total_loss': 100.01016235351562}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.912973880767822, 'policy_loss': 28.643720626831055, 'vf_loss': 118.27352142333984, 'total_loss': 87.71134948730469}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.920411109924316, 'policy_loss': 26.132896423339844, 'vf_loss': 114.05059051513672, 'total_loss': 83.08898162841797}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.822663307189941, 'policy_loss': -9.495055198669434, 'vf_loss': 20.08032989501953, 'total_loss': 0.4768831133842468}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.880149841308594, 'policy_loss': 9.633925437927246, 'vf_loss': 27.095230102539062, 'total_loss': 23.11273956298828}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.923653602600098, 'policy_loss': -25.43466567993164, 'vf_loss': 110.80110931396484, 'total_loss': 29.896652221679688}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.917377471923828, 'policy_loss': 35.63426971435547, 'vf_loss': 157.46340942382812, 'total_loss': 114.29679870605469}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.924718379974365, 'policy_loss': 33.263221740722656, 'vf_loss': 151.59291076660156, 'total_loss': 108.99043273925781}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.923868179321289, 'policy_loss': 33.55561447143555, 'vf_loss': 148.68324279785156, 'total_loss': 107.82799530029297}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.91841459274292, 'policy_loss': -14.922505378723145, 'vf_loss': 102.13048553466797, 'total_loss': 36.07355499267578}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.836842060089111, 'policy_loss': -9.29601764678955, 'vf_loss': 75.33857727050781, 'total_loss': 28.304903030395508}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.919955253601074, 'policy_loss': 33.57748031616211, 'vf_loss': 147.7480010986328, 'total_loss': 107.38227844238281}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.921798229217529, 'policy_loss': 32.01641082763672, 'vf_loss': 150.2381591796875, 'total_loss': 107.06626892089844}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.854527950286865, 'policy_loss': 11.243001937866211, 'vf_loss': 74.29721069335938, 'total_loss': 48.323062896728516}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.901895046234131, 'policy_loss': 22.803752899169922, 'vf_loss': 121.08140563964844, 'total_loss': 83.27543640136719}
No gradient information.s
training epoch 4 100 58.0 18.91
Main trainign step 5
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.90944242477417, 'policy_loss': -5.146950721740723, 'vf_loss': 138.1721954345703, 'total_loss': 63.870052337646484}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.911318302154541, 'policy_loss': 33.0501594543457, 'vf_loss': 149.71441650390625, 'total_loss': 107.83824920654297}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.905166149139404, 'policy_loss': 34.64608383178711, 'vf_loss': 146.93438720703125, 'total_loss': 108.04422760009766}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.903800010681152, 'policy_loss': -24.905996322631836, 'vf_loss': 118.99554443359375, 'total_loss': 34.522735595703125}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.8974528312683105, 'policy_loss': 34.38713073730469, 'vf_loss': 148.92123413085938, 'total_loss': 108.77877044677734}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.900290489196777, 'policy_loss': 32.378135681152344, 'vf_loss': 149.1757049560547, 'total_loss': 106.89698791503906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.897855758666992, 'policy_loss': -19.192626953125, 'vf_loss': 125.92809295654297, 'total_loss': 43.70244216918945}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.890607833862305, 'policy_loss': 34.067626953125, 'vf_loss': 148.37197875976562, 'total_loss': 108.18470764160156}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.8847856521606445, 'policy_loss': 12.832230567932129, 'vf_loss': 144.17596435546875, 'total_loss': 84.85136413574219}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.8862457275390625, 'policy_loss': 34.92675018310547, 'vf_loss': 148.39541625976562, 'total_loss': 109.05559539794922}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.888636112213135, 'policy_loss': -21.3687744140625, 'vf_loss': 126.68187713623047, 'total_loss': 41.90327835083008}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.891780376434326, 'policy_loss': 34.883304595947266, 'vf_loss': 147.61013793945312, 'total_loss': 108.61945343017578}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.900534629821777, 'policy_loss': 32.415565490722656, 'vf_loss': 147.57363891601562, 'total_loss': 106.13337707519531}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.904936790466309, 'policy_loss': 32.30254364013672, 'vf_loss': 147.656982421875, 'total_loss': 106.0619888305664}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.906835079193115, 'policy_loss': 21.441665649414062, 'vf_loss': 145.7897491455078, 'total_loss': 94.26747131347656}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.902615547180176, 'policy_loss': 31.85228157043457, 'vf_loss': 114.38975524902344, 'total_loss': 88.97813415527344}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.899680137634277, 'policy_loss': 33.407344818115234, 'vf_loss': 176.62423706054688, 'total_loss': 121.65046691894531}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.91121768951416, 'policy_loss': -23.52714729309082, 'vf_loss': 117.90315246582031, 'total_loss': 35.35531997680664}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.8138604164123535, 'policy_loss': -5.507305145263672, 'vf_loss': 32.14375686645508, 'total_loss': 10.496435165405273}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.918072700500488, 'policy_loss': -13.013676643371582, 'vf_loss': 135.69119262695312, 'total_loss': 54.76274108886719}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9168596267700195, 'policy_loss': 34.98469543457031, 'vf_loss': 150.91964721679688, 'total_loss': 110.37535095214844}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.92356538772583, 'policy_loss': -26.125123977661133, 'vf_loss': 126.78387451171875, 'total_loss': 37.19757843017578}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.925144195556641, 'policy_loss': 33.30686950683594, 'vf_loss': 148.5618133544922, 'total_loss': 107.51852416992188}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.925897121429443, 'policy_loss': -16.44005584716797, 'vf_loss': 127.88208770751953, 'total_loss': 47.43172836303711}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.901747226715088, 'policy_loss': 37.45737075805664, 'vf_loss': 210.33277893066406, 'total_loss': 142.55474853515625}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930052280426025, 'policy_loss': 33.44412612915039, 'vf_loss': 149.7228546142578, 'total_loss': 108.23625946044922}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930106163024902, 'policy_loss': 33.616798400878906, 'vf_loss': 148.816162109375, 'total_loss': 107.95558166503906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930362701416016, 'policy_loss': 33.54498291015625, 'vf_loss': 149.0011444091797, 'total_loss': 107.97624969482422}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.929805278778076, 'policy_loss': 30.014986038208008, 'vf_loss': 123.12821197509766, 'total_loss': 91.50979614257812}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.920081615447998, 'policy_loss': 28.270858764648438, 'vf_loss': 120.69391632080078, 'total_loss': 88.54861450195312}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.887638568878174, 'policy_loss': -5.0109076499938965, 'vf_loss': 72.73938751220703, 'total_loss': 31.28990936279297}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.8866472244262695, 'policy_loss': 10.804779052734375, 'vf_loss': 29.378087997436523, 'total_loss': 25.424957275390625}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.887969970703125, 'policy_loss': 17.12020492553711, 'vf_loss': 80.474853515625, 'total_loss': 57.288753509521484}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.900757312774658, 'policy_loss': -23.091215133666992, 'vf_loss': 66.21630859375, 'total_loss': 9.947931289672852}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9298601150512695, 'policy_loss': 31.63100242614746, 'vf_loss': 130.3598175048828, 'total_loss': 96.74161529541016}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.921996116638184, 'policy_loss': 20.457414627075195, 'vf_loss': 79.7059326171875, 'total_loss': 60.24115753173828}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.923811912536621, 'policy_loss': 22.778379440307617, 'vf_loss': 126.3192138671875, 'total_loss': 85.8687515258789}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9309821128845215, 'policy_loss': -13.867202758789062, 'vf_loss': 144.77066040039062, 'total_loss': 58.44881820678711}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.931026458740234, 'policy_loss': 33.57437515258789, 'vf_loss': 144.92076110839844, 'total_loss': 105.96544647216797}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.822663307189941, 'policy_loss': -9.495055198669434, 'vf_loss': 20.08032989501953, 'total_loss': 0.4768831133842468}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930269718170166, 'policy_loss': 32.521934509277344, 'vf_loss': 138.07122802734375, 'total_loss': 101.48824310302734}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.926285743713379, 'policy_loss': 34.65327072143555, 'vf_loss': 161.6016845703125, 'total_loss': 115.38485717773438}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.927276134490967, 'policy_loss': -10.83839225769043, 'vf_loss': 119.69287872314453, 'total_loss': 48.938777923583984}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.901838779449463, 'policy_loss': -16.750505447387695, 'vf_loss': 75.5362548828125, 'total_loss': 20.9486026763916}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862045764923096, 'policy_loss': -15.186519622802734, 'vf_loss': 37.516754150390625, 'total_loss': 3.503237009048462}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.919456958770752, 'policy_loss': 23.46367073059082, 'vf_loss': 79.80439758300781, 'total_loss': 63.29667282104492}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.893492698669434, 'policy_loss': -0.3908262252807617, 'vf_loss': 66.86994934082031, 'total_loss': 32.97521209716797}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930115699768066, 'policy_loss': 34.216041564941406, 'vf_loss': 159.32992553710938, 'total_loss': 113.81170654296875}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.92811918258667, 'policy_loss': 34.14732360839844, 'vf_loss': 147.36593627929688, 'total_loss': 107.7610092163086}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.927687168121338, 'policy_loss': -31.269941329956055, 'vf_loss': 140.87591552734375, 'total_loss': 39.09873580932617}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9132866859436035, 'policy_loss': 11.551909446716309, 'vf_loss': 29.74370765686035, 'total_loss': 26.354629516601562}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.881687164306641, 'policy_loss': 8.801868438720703, 'vf_loss': 73.90066528320312, 'total_loss': 45.68338394165039}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.915070533752441, 'policy_loss': 25.677682876586914, 'vf_loss': 84.91065979003906, 'total_loss': 68.06385803222656}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.854783535003662, 'policy_loss': -1.2521209716796875, 'vf_loss': 40.9539909362793, 'total_loss': 19.156326293945312}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.928544044494629, 'policy_loss': 33.250553131103516, 'vf_loss': 147.1736297607422, 'total_loss': 106.7680892944336}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.928678035736084, 'policy_loss': 33.18003845214844, 'vf_loss': 148.67095947265625, 'total_loss': 107.44622802734375}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.927270889282227, 'policy_loss': 31.74270248413086, 'vf_loss': 135.416748046875, 'total_loss': 99.38180541992188}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.927048683166504, 'policy_loss': 21.03095054626465, 'vf_loss': 68.10482025146484, 'total_loss': 55.01408767700195}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.901586532592773, 'policy_loss': -16.271827697753906, 'vf_loss': 56.91632080078125, 'total_loss': 12.117317199707031}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.923028945922852, 'policy_loss': 34.05986022949219, 'vf_loss': 147.33030700683594, 'total_loss': 107.6557846069336}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.924355506896973, 'policy_loss': 31.74957275390625, 'vf_loss': 139.33546447753906, 'total_loss': 101.34806060791016}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.870993137359619, 'policy_loss': 6.726768493652344, 'vf_loss': 15.953685760498047, 'total_loss': 14.63490104675293}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.821106910705566, 'policy_loss': 16.810237884521484, 'vf_loss': 55.83399200439453, 'total_loss': 44.65902328491211}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.922813415527344, 'policy_loss': 33.30269241333008, 'vf_loss': 145.64495849609375, 'total_loss': 106.05593872070312}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.898712635040283, 'policy_loss': -0.46639177203178406, 'vf_loss': 96.8194580078125, 'total_loss': 47.87434768676758}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.914763450622559, 'policy_loss': 36.234840393066406, 'vf_loss': 210.7688446044922, 'total_loss': 141.55010986328125}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.927326202392578, 'policy_loss': 14.334081649780273, 'vf_loss': 47.31427001953125, 'total_loss': 37.92194366455078}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.893792629241943, 'policy_loss': 14.648612022399902, 'vf_loss': 94.6504135131836, 'total_loss': 61.90488052368164}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 10.174028396606445, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930477619171143, 'policy_loss': 0.21392935514450073, 'vf_loss': 106.94619750976562, 'total_loss': 53.61772155761719}
> training_step. Has learer policy.
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.927276134490967, 'policy_loss': -10.83839225769043, 'vf_loss': 119.69287872314453, 'total_loss': 48.938777923583984}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.854621410369873, 'policy_loss': 34.20366668701172, 'vf_loss': 135.91131591796875, 'total_loss': 102.09078216552734}}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.832817554473877, 'policy_loss': 35.78886032104492, 'vf_loss': 163.90585327148438, 'total_loss': 117.6734619140625}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.865250110626221, 'policy_loss': 32.74784469604492, 'vf_loss': 147.9771728515625, 'total_loss': 106.66778564453125}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.877683639526367, 'policy_loss': -31.570627212524414, 'vf_loss': 165.0723419189453, 'total_loss': 50.89677047729492}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.859781265258789, 'policy_loss': 31.642606735229492, 'vf_loss': 133.56207275390625, 'total_loss': 98.35504913330078}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.785650253295898, 'policy_loss': -22.380775451660156, 'vf_loss': 84.99884796142578, 'total_loss': 20.050792694091797}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.860872268676758, 'policy_loss': 33.362369537353516, 'vf_loss': 171.8534393310547, 'total_loss': 119.2204818725586}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.894068241119385, 'policy_loss': 30.132736206054688, 'vf_loss': 138.6839599609375, 'total_loss': 99.40577697753906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.888051986694336, 'policy_loss': -7.985696792602539, 'vf_loss': 156.26361083984375, 'total_loss': 70.07723236083984}
No gradient information.s
training epoch 5 100 70.0 22.4
Main trainign step 6
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.874821662902832, 'policy_loss': 31.328981399536133, 'vf_loss': 138.22348022460938, 'total_loss': 100.3719711303711}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.884988307952881, 'policy_loss': 31.336341857910156, 'vf_loss': 146.77914428710938, 'total_loss': 104.65706634521484}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.881246566772461, 'policy_loss': 33.872772216796875, 'vf_loss': 151.1629180908203, 'total_loss': 109.38542175292969}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.898836612701416, 'policy_loss': 30.930194854736328, 'vf_loss': 142.50631713867188, 'total_loss': 102.11436462402344}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.886713027954102, 'policy_loss': 19.227014541625977, 'vf_loss': 151.9942169189453, 'total_loss': 95.1552505493164}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.880702018737793, 'policy_loss': 33.08181381225586, 'vf_loss': 145.04417419433594, 'total_loss': 105.53508758544922}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.873286247253418, 'policy_loss': -15.56191349029541, 'vf_loss': 173.5827178955078, 'total_loss': 71.16071319580078}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.919486045837402, 'policy_loss': -37.591983795166016, 'vf_loss': 203.522216796875, 'total_loss': 64.09992980957031}}}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 16.48101234436035, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.80764627456665, 'policy_loss': 0.45378291606903076, 'vf_loss': 9.737991333007812, 'total_loss': 5.254702091217041}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.672565460205078, 'policy_loss': 3.520157814025879, 'vf_loss': 36.32355880737305, 'total_loss': 21.61521339416504}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.7972798347473145, 'policy_loss': -3.454892158508301, 'vf_loss': 79.0034408569336, 'total_loss': 35.97885513305664}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.922593593597412, 'policy_loss': 32.40353012084961, 'vf_loss': 155.6309814453125, 'total_loss': 110.14979553222656}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.924770832061768, 'policy_loss': 33.09395980834961, 'vf_loss': 143.3502960205078, 'total_loss': 104.69985961914062}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9262614250183105, 'policy_loss': 31.43166732788086, 'vf_loss': 133.9680633544922, 'total_loss': 98.346435546875}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.923202037811279, 'policy_loss': -2.124763011932373, 'vf_loss': 178.88275146484375, 'total_loss': 87.24738311767578}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.685361385345459, 'policy_loss': -19.289447784423828, 'vf_loss': 53.90180587768555, 'total_loss': 7.594601631164551}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.917706489562988, 'policy_loss': 34.41529846191406, 'vf_loss': 159.875732421875, 'total_loss': 114.28398895263672}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.928928375244141, 'policy_loss': 33.04214859008789, 'vf_loss': 144.07388305664062, 'total_loss': 105.00979614257812}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930071830749512, 'policy_loss': 31.853548049926758, 'vf_loss': 134.5924835205078, 'total_loss': 99.08049011230469}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.92979621887207, 'policy_loss': 7.507299423217773, 'vf_loss': 170.66946411132812, 'total_loss': 92.77273559570312}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.926975250244141, 'policy_loss': 33.659122467041016, 'vf_loss': 141.57611083984375, 'total_loss': 104.3779067993164}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.805983543395996, 'policy_loss': 13.95594310760498, 'vf_loss': 29.759693145751953, 'total_loss': 28.767728805541992}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.721588134765625, 'policy_loss': -4.65484619140625, 'vf_loss': 71.46522521972656, 'total_loss': 31.01055145263672}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.928921699523926, 'policy_loss': 32.99161911010742, 'vf_loss': 141.6117401123047, 'total_loss': 103.72819519042969}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9301862716674805, 'policy_loss': 32.845664978027344, 'vf_loss': 141.56214904785156, 'total_loss': 103.55743408203125}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930627822875977, 'policy_loss': 32.800331115722656, 'vf_loss': 140.62379455566406, 'total_loss': 103.04292297363281}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930173397064209, 'policy_loss': 25.877971649169922, 'vf_loss': 97.5408935546875, 'total_loss': 74.57911682128906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.674036026000977, 'policy_loss': -29.434579849243164, 'vf_loss': 74.99124908447266, 'total_loss': 7.994304180145264}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.796638011932373, 'policy_loss': 26.315561294555664, 'vf_loss': 88.29460906982422, 'total_loss': 70.39490509033203}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.931267738342285, 'policy_loss': 32.73432922363281, 'vf_loss': 140.56175231933594, 'total_loss': 102.94589233398438}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.926013946533203, 'policy_loss': 33.43585205078125, 'vf_loss': 141.26510620117188, 'total_loss': 103.9991455078125}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.92978048324585, 'policy_loss': 33.134864807128906, 'vf_loss': 141.58790588378906, 'total_loss': 103.8595199584961}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930039882659912, 'policy_loss': -27.486186981201172, 'vf_loss': 217.78529357910156, 'total_loss': 81.33716583251953}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.928708076477051, 'policy_loss': 28.930212020874023, 'vf_loss': 112.53773498535156, 'total_loss': 85.12979125976562}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.929315567016602, 'policy_loss': 14.326336860656738, 'vf_loss': 119.89171600341797, 'total_loss': 74.2029037475586}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.918248653411865, 'policy_loss': 33.819969177246094, 'vf_loss': 160.673828125, 'total_loss': 114.08769989013672}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930124282836914, 'policy_loss': 32.913631439208984, 'vf_loss': 147.01686096191406, 'total_loss': 106.3527603149414}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.931213855743408, 'policy_loss': 30.560924530029297, 'vf_loss': 128.3071746826172, 'total_loss': 94.64519500732422}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.67669677734375, 'policy_loss': -18.229589462280273, 'vf_loss': 56.795291900634766, 'total_loss': 10.101289749145508}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.920363426208496, 'policy_loss': 32.75717544555664, 'vf_loss': 134.46664428710938, 'total_loss': 99.9212875366211}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.877227783203125, 'policy_loss': 37.01811218261719, 'vf_loss': 200.21168518066406, 'total_loss': 137.05519104003906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9201130867004395, 'policy_loss': 9.766234397888184, 'vf_loss': 36.02136993408203, 'total_loss': 27.707719802856445}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.703811168670654, 'policy_loss': -0.41671156883239746, 'vf_loss': 54.64223098754883, 'total_loss': 26.837366104125977}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.726445198059082, 'policy_loss': 41.20836639404297, 'vf_loss': 279.0472717285156, 'total_loss': 180.66473388671875}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.906637668609619, 'policy_loss': -2.0447444915771484, 'vf_loss': 157.75758361816406, 'total_loss': 76.76497650146484}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.921980857849121, 'policy_loss': -11.87498950958252, 'vf_loss': 196.98471069335938, 'total_loss': 86.54814147949219}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.919826984405518, 'policy_loss': 32.58864212036133, 'vf_loss': 144.19369506835938, 'total_loss': 104.61628723144531}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.917572021484375, 'policy_loss': -44.27401351928711, 'vf_loss': 248.6897735595703, 'total_loss': 80.00170135498047}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.920607089996338, 'policy_loss': -36.65080261230469, 'vf_loss': 221.06048583984375, 'total_loss': 73.81023406982422}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.7891082763671875, 'policy_loss': 40.900482177734375, 'vf_loss': 212.69216918945312, 'total_loss': 147.17868041992188}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.742923259735107, 'policy_loss': -20.35430908203125, 'vf_loss': 88.16376495361328, 'total_loss': 23.660144805908203}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 27.92624282836914, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.795398235321045, 'policy_loss': -1.8216297626495361, 'vf_loss': 15.91268539428711, 'total_loss': 6.06675910949707}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.67520809173584, 'policy_loss': 12.406404495239258, 'vf_loss': 50.14244842529297, 'total_loss': 37.4108772277832}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.922736167907715, 'policy_loss': -37.8320426940918, 'vf_loss': 231.76181030273438, 'total_loss': 77.97962951660156}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.912116050720215, 'policy_loss': 31.785011291503906, 'vf_loss': 136.6374053955078, 'total_loss': 100.03459167480469}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.913511753082275, 'policy_loss': -28.566511154174805, 'vf_loss': 216.29751586914062, 'total_loss': 79.51310729980469}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9052414894104, 'policy_loss': 28.479839324951172, 'vf_loss': 130.68618774414062, 'total_loss': 93.7538833618164}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.919528484344482, 'policy_loss': -31.877586364746094, 'vf_loss': 214.7233123779297, 'total_loss': 75.41487121582031}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.909581184387207, 'policy_loss': 30.211334228515625, 'vf_loss': 130.20579528808594, 'total_loss': 95.24513244628906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.912742614746094, 'policy_loss': -4.963596343994141, 'vf_loss': 198.05047607421875, 'total_loss': 93.99251556396484}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.893367290496826, 'policy_loss': -12.354288101196289, 'vf_loss': 184.66693115234375, 'total_loss': 79.91024780273438}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.8804240226745605, 'policy_loss': 24.81416130065918, 'vf_loss': 107.67757415771484, 'total_loss': 78.58414459228516}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.8522629737854, 'policy_loss': 32.55904006958008, 'vf_loss': 164.1552734375, 'total_loss': 114.56815338134766}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.886677265167236, 'policy_loss': 27.44369888305664, 'vf_loss': 110.83015441894531, 'total_loss': 82.78990936279297}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.919486045837402, 'policy_loss': -37.591983795166016, 'vf_loss': 203.522216796875, 'total_loss': 64.09992980957031}}}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.874870300292969, 'policy_loss': 29.237789154052734, 'vf_loss': 137.43429565429688, 'total_loss': 97.88619232177734}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.873584270477295, 'policy_loss': 31.450664520263672, 'vf_loss': 127.23161315917969, 'total_loss': 94.99773406982422}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.8640618324279785, 'policy_loss': 16.442886352539062, 'vf_loss': 152.25152587890625, 'total_loss': 92.50000762939453}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.881196975708008, 'policy_loss': -30.797616958618164, 'vf_loss': 210.59336853027344, 'total_loss': 74.43025970458984}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.86264181137085, 'policy_loss': 30.216873168945312, 'vf_loss': 132.22787475585938, 'total_loss': 96.2621841430664}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.872519493103027, 'policy_loss': -18.570199966430664, 'vf_loss': 197.53195190429688, 'total_loss': 80.1270523071289}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.839602947235107, 'policy_loss': 33.11885452270508, 'vf_loss': 139.92025756835938, 'total_loss': 103.01058197021484}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.84333610534668, 'policy_loss': -47.66742706298828, 'vf_loss': 264.04327392578125, 'total_loss': 84.28577423095703}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.807849407196045, 'policy_loss': 29.81686782836914, 'vf_loss': 110.42448425292969, 'total_loss': 84.9610366821289}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.827515602111816, 'policy_loss': 23.666685104370117, 'vf_loss': 109.68755340576172, 'total_loss': 78.44218444824219}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.863905429840088, 'policy_loss': 36.554954528808594, 'vf_loss': 163.5381317138672, 'total_loss': 118.25537872314453}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.864620208740234, 'policy_loss': 33.391021728515625, 'vf_loss': 143.42416381835938, 'total_loss': 105.03445434570312}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.853738307952881, 'policy_loss': -2.5660250186920166, 'vf_loss': 196.1523895263672, 'total_loss': 95.44163513183594}
No gradient information.s
training epoch 6 100 70.0 24.11
Main trainign step 7
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.846981525421143, 'policy_loss': 33.349754333496094, 'vf_loss': 140.1856231689453, 'total_loss': 103.37409973144531}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.894730567932129, 'policy_loss': -41.800601959228516, 'vf_loss': 227.90255737304688, 'total_loss': 72.08173370361328}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.902410507202148, 'policy_loss': -38.57118225097656, 'vf_loss': 216.1461639404297, 'total_loss': 69.43287658691406}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.451725006103516, 'policy_loss': 24.440719604492188, 'vf_loss': 72.68885040283203, 'total_loss': 60.72062683105469}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.791675567626953, 'policy_loss': 11.710137367248535, 'vf_loss': 102.23692321777344, 'total_loss': 62.76068115234375}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.871252536773682, 'policy_loss': 31.4268856048584, 'vf_loss': 157.82183837890625, 'total_loss': 110.26909637451172}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.902892112731934, 'policy_loss': 27.938579559326172, 'vf_loss': 104.69644165039062, 'total_loss': 80.2177734375}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.920427322387695, 'policy_loss': 35.769107818603516, 'vf_loss': 169.79286193847656, 'total_loss': 120.5963363647461}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.911909580230713, 'policy_loss': -48.49522399902344, 'vf_loss': 271.1454772949219, 'total_loss': 87.00839233398438}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.898176193237305, 'policy_loss': 33.266326904296875, 'vf_loss': 142.21646118164062, 'total_loss': 104.30557250976562}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.912331581115723, 'policy_loss': 31.629423141479492, 'vf_loss': 139.43948364257812, 'total_loss': 101.28004455566406}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.919816017150879, 'policy_loss': 32.77803421020508, 'vf_loss': 144.82017517089844, 'total_loss': 105.11892700195312}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.911981582641602, 'policy_loss': 33.47905349731445, 'vf_loss': 138.89813232421875, 'total_loss': 102.85899353027344}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.898341655731201, 'policy_loss': 33.47711944580078, 'vf_loss': 148.30323791503906, 'total_loss': 107.55975341796875}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9157304763793945, 'policy_loss': -41.95396423339844, 'vf_loss': 229.5913848876953, 'total_loss': 72.77256774902344}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.914413928985596, 'policy_loss': 30.05939483642578, 'vf_loss': 125.932373046875, 'total_loss': 92.95643615722656}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9047675132751465, 'policy_loss': 24.540010452270508, 'vf_loss': 98.6240234375, 'total_loss': 73.78297424316406}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.902385711669922, 'policy_loss': 31.089563369750977, 'vf_loss': 157.00839233398438, 'total_loss': 109.52473449707031}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.923483848571777, 'policy_loss': 32.19111633300781, 'vf_loss': 140.49703979492188, 'total_loss': 102.37039947509766}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.925010681152344, 'policy_loss': -46.17299270629883, 'vf_loss': 262.3927001953125, 'total_loss': 84.95410919189453}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930412769317627, 'policy_loss': 32.7335319519043, 'vf_loss': 139.83401489257812, 'total_loss': 102.58123779296875}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9304986000061035, 'policy_loss': 32.49537658691406, 'vf_loss': 138.48562622070312, 'total_loss': 101.66888427734375}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.930045127868652, 'policy_loss': 26.896278381347656, 'vf_loss': 101.46337890625, 'total_loss': 77.55867004394531}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.633544921875, 'policy_loss': -19.271299362182617, 'vf_loss': 68.82759094238281, 'total_loss': 15.076160430908203}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.771366596221924, 'policy_loss': 38.633235931396484, 'vf_loss': 199.49041748046875, 'total_loss': 138.31072998046875}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.927458763122559, 'policy_loss': 28.680461883544922, 'vf_loss': 117.94464874267578, 'total_loss': 87.58351135253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.92044734954834, 'policy_loss': 29.25458526611328, 'vf_loss': 115.0870361328125, 'total_loss': 86.72889709472656}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.884571552276611, 'policy_loss': 36.271114349365234, 'vf_loss': 156.69349670410156, 'total_loss': 114.54901123046875}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.837005615234375, 'policy_loss': -27.435970306396484, 'vf_loss': 135.82960510253906, 'total_loss': 40.41046142578125}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.925151824951172, 'policy_loss': 32.01622772216797, 'vf_loss': 134.3057861328125, 'total_loss': 99.09986877441406}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.913135528564453, 'policy_loss': 27.115610122680664, 'vf_loss': 101.18832397460938, 'total_loss': 77.64064025878906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.879875183105469, 'policy_loss': -15.057768821716309, 'vf_loss': 185.58909606933594, 'total_loss': 77.66797637939453}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.917799472808838, 'policy_loss': 32.791656494140625, 'vf_loss': 147.0165252685547, 'total_loss': 106.23074340820312}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.927948951721191, 'policy_loss': -23.84452247619629, 'vf_loss': 257.44219970703125, 'total_loss': 104.80729675292969}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.89823055267334, 'policy_loss': 25.718524932861328, 'vf_loss': 88.21247863769531, 'total_loss': 69.75578308105469}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.5194573402404785, 'policy_loss': -7.679537773132324, 'vf_loss': 67.44776153564453, 'total_loss': 25.97914695739746}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.829762935638428, 'policy_loss': 25.720420837402344, 'vf_loss': 105.71915435791016, 'total_loss': 78.51170349121094}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.913132190704346, 'policy_loss': 33.97534942626953, 'vf_loss': 153.37014770507812, 'total_loss': 110.59129333496094}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.919793128967285, 'policy_loss': 3.792954921722412, 'vf_loss': 176.86354064941406, 'total_loss': 92.15552520751953}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.928418159484863, 'policy_loss': 32.082698822021484, 'vf_loss': 145.41319274902344, 'total_loss': 104.72000885009766}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.926008224487305, 'policy_loss': 30.839710235595703, 'vf_loss': 130.46090698242188, 'total_loss': 96.00090026855469}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.902584075927734, 'policy_loss': -44.99566650390625, 'vf_loss': 242.6320037841797, 'total_loss': 76.25131225585938}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.857288360595703, 'policy_loss': 36.75249481201172, 'vf_loss': 174.15394592285156, 'total_loss': 123.76089477539062}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.734755992889404, 'policy_loss': -40.18718719482422, 'vf_loss': 157.71798706054688, 'total_loss': 38.60445785522461}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.889095783233643, 'policy_loss': 13.6292724609375, 'vf_loss': 38.27007293701172, 'total_loss': 32.69541931152344}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.699073791503906, 'policy_loss': -26.547168731689453, 'vf_loss': 168.2122039794922, 'total_loss': 57.491943359375}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9233222007751465, 'policy_loss': 27.82283592224121, 'vf_loss': 108.93115234375, 'total_loss': 82.21917724609375}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.916108131408691, 'policy_loss': -39.43730163574219, 'vf_loss': 256.472412109375, 'total_loss': 88.72974395751953}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.919575214385986, 'policy_loss': 32.431427001953125, 'vf_loss': 137.8916778564453, 'total_loss': 101.30806732177734}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.8294997215271, 'policy_loss': 16.631736755371094, 'vf_loss': 43.36061477661133, 'total_loss': 38.24374771118164}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.7728095054626465, 'policy_loss': 31.915010452270508, 'vf_loss': 212.9758758544922, 'total_loss': 138.33522033691406}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9076104164123535, 'policy_loss': 18.76284408569336, 'vf_loss': 164.61962890625, 'total_loss': 101.00358581542969}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.918239116668701, 'policy_loss': 32.39030838012695, 'vf_loss': 138.96246337890625, 'total_loss': 101.80235290527344}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9185709953308105, 'policy_loss': 32.41813278198242, 'vf_loss': 139.695556640625, 'total_loss': 102.19673156738281}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.905645370483398, 'policy_loss': 32.77137756347656, 'vf_loss': 131.8915252685547, 'total_loss': 98.64808654785156}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.865380764007568, 'policy_loss': 34.46753692626953, 'vf_loss': 170.2554473876953, 'total_loss': 119.52660369873047}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 29.367141723632812, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.844610214233398, 'policy_loss': 3.408980131149292, 'vf_loss': 17.817842483520508, 'total_loss': 12.249455451965332}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.844901084899902, 'policy_loss': 24.47589683532715, 'vf_loss': 124.72856903076172, 'total_loss': 86.771728515625}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.70020055770874, 'policy_loss': -1.8353602886199951, 'vf_loss': 11.337713241577148, 'total_loss': 3.7664942741394043}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.645474433898926, 'policy_loss': 8.292141914367676, 'vf_loss': 83.50542449951172, 'total_loss': 49.978397369384766}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.904324054718018, 'policy_loss': 32.42657470703125, 'vf_loss': 137.83712768554688, 'total_loss': 101.27609252929688}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.908758640289307, 'policy_loss': -45.856204986572266, 'vf_loss': 287.16802978515625, 'total_loss': 97.65872955322266}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.471348762512207, 'policy_loss': -34.08357620239258, 'vf_loss': 92.29485321044922, 'total_loss': 11.999136924743652}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.872257232666016, 'policy_loss': 33.555564880371094, 'vf_loss': 150.4954376220703, 'total_loss': 108.73455810546875}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.865502834320068, 'policy_loss': 29.7612247467041, 'vf_loss': 127.24408721923828, 'total_loss': 93.31461334228516}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.738193511962891, 'policy_loss': -18.26634407043457, 'vf_loss': 161.3626251220703, 'total_loss': 62.34758758544922}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.905405044555664, 'policy_loss': 33.61830520629883, 'vf_loss': 141.1377716064453, 'total_loss': 104.1181411743164}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.870853900909424, 'policy_loss': -2.596127986907959, 'vf_loss': 232.86825561523438, 'total_loss': 113.769287109375}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.891596794128418, 'policy_loss': 32.28549575805664, 'vf_loss': 139.3462371826172, 'total_loss': 101.88970184326172}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.916144371032715, 'policy_loss': 32.455482482910156, 'vf_loss': 148.71987915039062, 'total_loss': 106.74626159667969}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.910087585449219, 'policy_loss': -15.49884033203125, 'vf_loss': 220.04124450683594, 'total_loss': 94.45268249511719}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.902116775512695, 'policy_loss': 23.77351188659668, 'vf_loss': 76.22731018066406, 'total_loss': 61.81814956665039}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.879119396209717, 'policy_loss': 18.898954391479492, 'vf_loss': 156.49493408203125, 'total_loss': 97.07762908935547}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.8952531814575195, 'policy_loss': 25.382532119750977, 'vf_loss': 92.47161865234375, 'total_loss': 71.54938507080078}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.485892295837402, 'policy_loss': -32.052101135253906, 'vf_loss': 117.53692626953125, 'total_loss': 26.65150260925293}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.905402660369873, 'policy_loss': 32.447547912597656, 'vf_loss': 138.1492462158203, 'total_loss': 101.45311737060547}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.907155990600586, 'policy_loss': 29.46616554260254, 'vf_loss': 115.45516967773438, 'total_loss': 87.12467956542969}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.919022560119629, 'policy_loss': 32.60367965698242, 'vf_loss': 139.69883728027344, 'total_loss': 102.38390350341797}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.916463851928711, 'policy_loss': -39.10123062133789, 'vf_loss': 281.328857421875, 'total_loss': 101.49403381347656}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.8463921546936035, 'policy_loss': 34.200157165527344, 'vf_loss': 151.02378845214844, 'total_loss': 109.64358520507812}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.918912410736084, 'policy_loss': 31.942087173461914, 'vf_loss': 147.83302307128906, 'total_loss': 105.7894058227539}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.901836395263672, 'policy_loss': -40.420066833496094, 'vf_loss': 271.93975830078125, 'total_loss': 95.48079681396484}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.928524971008301, 'policy_loss': 30.8841495513916, 'vf_loss': 131.96693420410156, 'total_loss': 96.79833221435547}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.923142433166504, 'policy_loss': 31.77448844909668, 'vf_loss': 140.24716186523438, 'total_loss': 101.82884216308594}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.926502227783203, 'policy_loss': 32.6611213684082, 'vf_loss': 146.13751220703125, 'total_loss': 105.66060638427734}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.9275665283203125, 'policy_loss': 24.828441619873047, 'vf_loss': 90.93920135498047, 'total_loss': 70.22876739501953}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.91217041015625, 'policy_loss': 31.613115310668945, 'vf_loss': 158.08468627929688, 'total_loss': 110.58633422851562}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.908761024475098, 'policy_loss': -53.7711296081543, 'vf_loss': 331.025634765625, 'total_loss': 111.67259979248047}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.929253578186035, 'policy_loss': 32.100318908691406, 'vf_loss': 140.94766235351562, 'total_loss': 102.5048599243164}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.921319007873535, 'policy_loss': 33.542030334472656, 'vf_loss': 159.3245849609375, 'total_loss': 113.1351089477539}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.865044116973877, 'policy_loss': -16.692750930786133, 'vf_loss': 190.147705078125, 'total_loss': 78.31245422363281}
> training_step. Has learer policy.
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.913132190704346, 'policy_loss': 33.97534942626953, 'vf_loss': 153.37014770507812, 'total_loss': 110.59129333496094}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
[33m(raylet)[39m [2022-09-30 17:03:34,533 E 42920 42952] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2022-09-30_17-02-20_910330_42836 is over 95% full, available space: 4570234880; capacity: 125844406272. Object creation will fail if spilling is required.
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
[33m(raylet)[39m [2022-09-30 17:03:44,550 E 42920 42952] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2022-09-30_17-02-20_910330_42836 is over 95% full, available space: 4570439680; capacity: 125844406272. Object creation will fail if spilling is required.
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
[33m(raylet)[39m [2022-09-30 17:03:54,565 E 42920 42952] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2022-09-30_17-02-20_910330_42836 is over 95% full, available space: 4569903104; capacity: 125844406272. Object creation will fail if spilling is required.
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
[33m(raylet)[39m [2022-09-30 17:04:04,575 E 42920 42952] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2022-09-30_17-02-20_910330_42836 is over 95% full, available space: 4569292800; capacity: 125844406272. Object creation will fail if spilling is required.
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
[33m(raylet)[39m [2022-09-30 17:04:14,586 E 42920 42952] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2022-09-30_17-02-20_910330_42836 is over 95% full, available space: 4568662016; capacity: 125844406272. Object creation will fail if spilling is required.
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
[33m(raylet)[39m [2022-09-30 17:04:24,596 E 42920 42952] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2022-09-30_17-02-20_910330_42836 is over 95% full, available space: 4567969792; capacity: 125844406272. Object creation will fail if spilling is required.
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
[33m(raylet)[39m [2022-09-30 17:04:34,610 E 42920 42952] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2022-09-30_17-02-20_910330_42836 is over 95% full, available space: 4567343104; capacity: 125844406272. Object creation will fail if spilling is required.
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
[33m(raylet)[39m [2022-09-30 17:04:44,623 E 42920 42952] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2022-09-30_17-02-20_910330_42836 is over 95% full, available space: 4566712320; capacity: 125844406272. Object creation will fail if spilling is required.
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
[33m(raylet)[39m [2022-09-30 17:04:54,640 E 42920 42952] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2022-09-30_17-02-20_910330_42836 is over 95% full, available space: 4566065152; capacity: 125844406272. Object creation will fail if spilling is required.
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
[33m(raylet)[39m [2022-09-30 17:05:04,649 E 42920 42952] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2022-09-30_17-02-20_910330_42836 is over 95% full, available space: 4565372928; capacity: 125844406272. Object creation will fail if spilling is required.
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
[33m(raylet)[39m [2022-09-30 17:05:14,658 E 42920 42952] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2022-09-30_17-02-20_910330_42836 is over 95% full, available space: 4564668416; capacity: 125844406272. Object creation will fail if spilling is required.
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}
> training_step. Has learer policy.  {'allreduce_latency': 0.0, 'grad_gnorm': 30.0, 'cur_lr': 0.001, 'entropy_coeff': 0.01, 'policy_entropy': 6.862061500549316, 'policy_loss': -36.71779251098633, 'vf_loss': 312.7476501464844, 'total_loss': 119.58741760253906}